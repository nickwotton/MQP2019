{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BSM_NN_v01.ipynb ",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nickwotton/MQP2019/blob/master/Nick/BSM_NN_v01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pu5uIyYP6Tnz",
        "colab_type": "text"
      },
      "source": [
        "# Attempt to Replicate the Black Scholes Model Using a Neural Network\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KOvg3FsM7r51",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.stats as ss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4PCo06270zm",
        "colab_type": "text"
      },
      "source": [
        "## Define the Function\n",
        "Here we define our function, the Black Scholes Model (BSM). First, we must initialize the option class, then the Geometric Brownian Motion Class, and finally the BSM class.\n",
        "Then we test the equation with a test value of 2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wdyGtXXn7r56",
        "colab": {}
      },
      "source": [
        "'''=========\n",
        "option class init\n",
        "=========='''\n",
        "class VanillaOption:\n",
        "    def __init__(\n",
        "        self,\n",
        "        otype = 1, # 1: 'call'\n",
        "                  # -1: 'put'\n",
        "        strike = 110.,\n",
        "        maturity = 1.,\n",
        "        market_price = 10.):\n",
        "      self.otype = otype               # Put or Call\n",
        "      self.strike = strike             # Strike K\n",
        "      self.maturity = maturity         # Maturity T\n",
        "      self.market_price = market_price #this will be used for calibration\n",
        "      \n",
        "        \n",
        "    def payoff(self, s): #s: excercise price\n",
        "      otype = self.otype\n",
        "      k = self.strike\n",
        "      maturity = self.maturity\n",
        "      return np.max([0, (s - k)*otype])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ZT5HETX8jIs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''============\n",
        "Gbm class\n",
        "============='''\n",
        "\n",
        "class Gbm:\n",
        "    def __init__(self,\n",
        "                 init_state = 100.,\n",
        "                 drift_ratio = .0475,\n",
        "                 vol_ratio = .2\n",
        "                ):\n",
        "        self.init_state = init_state\n",
        "        self.drift_ratio = drift_ratio\n",
        "        self.vol_ratio = vol_ratio"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dkd7sAZCEYUk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''========\n",
        "Black-Scholes-Merton formula. \n",
        "=========='''\n",
        "\n",
        "def bsm_price(self, vanilla_option):\n",
        "    s0 = self.init_state\n",
        "    sigma = self.vol_ratio\n",
        "    r = self.drift_ratio\n",
        "    \n",
        "    otype = vanilla_option.otype\n",
        "    k = vanilla_option.strike\n",
        "    maturity = vanilla_option.maturity\n",
        "    \n",
        "    d1 = 1/(sigma*np.sqrt(maturity))*(np.log(s0/k) + (r + np.power(sigma,2)/2)*(maturity)) \n",
        "    d2 = 1/(sigma*np.sqrt(maturity))*(np.log(s0/k) + (r - np.power(sigma,2)/2)*(maturity)) \n",
        "    return (otype * s0 * ss.norm.cdf(otype * d1) #line break needs parenthesis\n",
        "            - otype * np.exp(-r * maturity) * k * ss.norm.cdf(otype * d2))\n",
        "\n",
        "Gbm.bsm_price = bsm_price"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vC1nkSnVIBgi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''=======\n",
        "Get BSM prices given an option and a vector\n",
        "======='''\n",
        "\n",
        "def vector_bsm(self, vanilla_option, data):\n",
        "  outputData = []\n",
        "  for i in data:\n",
        "    gbm1.init_state = i\n",
        "    callPrice = gbm1.bsm_price(vanilla_option)\n",
        "    outputData.append(callPrice)\n",
        "  return outputData\n",
        "\n",
        "Gbm.vector_bsm = vector_bsm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7G7JVcog6Ref",
        "colab_type": "text"
      },
      "source": [
        "## Create Model\n",
        "Next, we create the neural network model. This is done first by setting the inner and outer dimensions with variables. Next we code the model and vary the internal dimensions to attempt to improve the model. At this level, this is essentially a simple linear algebra exercise:\n",
        "\n",
        "If we have input $x$, internal parameters $a,b$, and solution $f(x)$ then in the one-dimensional case we have:\n",
        "\n",
        "\\begin{equation}\n",
        "  \\left(\n",
        "    a_{1}x+b_{2}\n",
        "  \\right)\n",
        "  a_{2} + b_{2}\n",
        "  = f(x)\n",
        "\\end{equation}\n",
        "  \n",
        "However, we want to get a better estimate for the true equation. So we increase the interior dimension which corresponds to the number of neurons inside the network. For example, we raised the inner dimension to 3. In matrix form we have:\n",
        "\n",
        "\\begin{equation}\n",
        "\\left(\n",
        "  \\begin{bmatrix} x \\end{bmatrix} \n",
        "  \\begin{bmatrix} a_{1} & a_{2} & a_{3} \\end{bmatrix} \n",
        "  + \n",
        "  \\begin{bmatrix} b_{1} & b_{2} & b_{3} \\end{bmatrix}\n",
        "\\right)\n",
        " \\begin{bmatrix} a_{4} \\\\ a_{5} \\\\ a_{6} \\end{bmatrix}\n",
        " +\n",
        " \\begin{bmatrix} b_{4} \\\\ \\end{bmatrix}\n",
        " =\n",
        " \\begin{bmatrix} f(x) \\end{bmatrix}\n",
        "\\end{equation}\n",
        "\n",
        "Graphically, we can render this second neural network as:\n",
        "![Neural Network Diagram](https://drive.google.com/uc?id=1ItiBpdjPvWHF5ZWy8JDNDKq6dXfyU-IE)\n",
        "\n",
        "What we discovered here is that ReLU was slowing down the process, so since our function is Linear, we can just remove it. \n",
        "\n",
        "Additionally, we discerned that the higher the inner dimension, that is, the more nodes in each layer, the smaller the error and the better the performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CiVUxudQ7r5_",
        "colab": {}
      },
      "source": [
        "#model\n",
        "#nn.Linear\n",
        "in_dim = 1\n",
        "out_dim = 1\n",
        "int_dim = 10\n",
        "\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(in_dim, int_dim),\n",
        "    nn.ReLU(),\n",
        "    #nn.Linear(int_dim, int_dim),\n",
        "    #nn.ReLU(),\n",
        "    nn.Linear(int_dim, out_dim)\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkuS30xvzTgV",
        "colab_type": "text"
      },
      "source": [
        "Here we define the Loss function as the Mean Squared Error(MSE). \n",
        "\n",
        "Note that by doing so, we are essentially 'cheating' the system. In most applications, we would not know the function $f$ so we would be unable to find the MSE."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7TqjOaLs7r6C",
        "colab": {}
      },
      "source": [
        "#loss function \n",
        "criterion = nn.MSELoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIblNHIrzt8f",
        "colab_type": "text"
      },
      "source": [
        "Next we choose a learning rate and a method for learning. The learning rate is the percent of the data that is accepted in each iteration. The Methods we tried were SGD and Adam. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "E4rnJCtA7r6G",
        "colab": {}
      },
      "source": [
        "#optimizer\n",
        "learning_rate = 0.001\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-FHbIND2A34",
        "colab_type": "text"
      },
      "source": [
        "## Train the Model\n",
        "\n",
        "First we create the training data. This is a batch of random points that we pass through the BSM."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SbutUBbo7r6K",
        "colab": {}
      },
      "source": [
        "#training data\n",
        "\n",
        "batch_size = 1000\n",
        "\n",
        "x_trainPy = torch.randn(batch_size, 1)\n",
        "#print(x_trainPy)\n",
        "x_trainFlat = torch.flatten(x_trainPy)\n",
        "#print(x_trainFlat)\n",
        "x_train = np.abs(x_trainFlat.tolist())            # Convert to a list for ease of use, Take absolute value since we need stock prices > 0\n",
        "#print(x_train)\n",
        "x_trainScaled = [x * 100 for x in x_train]        # Scale list to make valid stock prices\n",
        "#print(x_trainScaled)\n",
        "gbm1 = Gbm()\n",
        "option1 = VanillaOption()\n",
        "y_train = gbm1.vector_bsm(option1, x_trainScaled)\n",
        "#print(y_train)\n",
        "#for i in x_trainScaled:\n",
        " # gbm1.init_state = i\n",
        "  #callPrice = gbm1.bsm_price(option1)\n",
        "  #y_train.append(callPrice)\n",
        "\n",
        "#print(x_train)\n",
        "#print(x_trainScaled)\n",
        "#print(y_train)\n",
        "\n",
        "x_trainTensor = torch.FloatTensor(x_trainScaled)  # Convert the data to a Tensor again for use in training the model\n",
        "x_trainRotTensor = x_trainTensor.unsqueeze(1)     # Transpose the tensor to make it into the form we need for training\n",
        "\n",
        "y_trainTensor = torch.FloatTensor(y_train)\n",
        "y_trainRotTensor = y_trainTensor.unsqueeze(1)\n",
        "\n",
        "#print(x_trainTensor)\n",
        "#print(x_trainRotTensor)\n",
        "#print(y_trainTensor)\n",
        "#print(y_trainRotTensor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9VZMG4e2ziF",
        "colab_type": "text"
      },
      "source": [
        "Once we have the training data, we pass this collection of inputs and solutions into the model. With each iteration we calculate the loss and attempt to optimize the model to further reduce the loss.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4hNIFisK7r6M",
        "outputId": "3f3c32c5-8e0d-4102-b694-518ac1f686cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        }
      },
      "source": [
        "# Train the model\n",
        "\n",
        "num_epochs = 1000\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    # Forward pass\n",
        "    outputs = model(x_trainRotTensor)\n",
        "    loss = criterion(outputs, y_trainRotTensor)\n",
        "    \n",
        "    # Backward and optimize\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    if (epoch+1) % 50 == 0:\n",
        "        print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, \n",
        "                                                    num_epochs, loss.item()))\n",
        "        #print(x_trainRotTensor[0:10])\n",
        "        #print(outputs[0:10])\n",
        "        #print(y_trainRotTensor[0:10])\n"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch [50/1000], Loss: 1208.3547\n",
            "Epoch [100/1000], Loss: 1207.5942\n",
            "Epoch [150/1000], Loss: 1206.9716\n",
            "Epoch [200/1000], Loss: 1206.4617\n",
            "Epoch [250/1000], Loss: 1206.0442\n",
            "Epoch [300/1000], Loss: 1205.7023\n",
            "Epoch [350/1000], Loss: 1205.4222\n",
            "Epoch [400/1000], Loss: 1205.1929\n",
            "Epoch [450/1000], Loss: 1205.0050\n",
            "Epoch [500/1000], Loss: 1204.8511\n",
            "Epoch [550/1000], Loss: 1204.7250\n",
            "Epoch [600/1000], Loss: 1204.6216\n",
            "Epoch [650/1000], Loss: 1204.5369\n",
            "Epoch [700/1000], Loss: 1204.4674\n",
            "Epoch [750/1000], Loss: 1204.4105\n",
            "Epoch [800/1000], Loss: 1204.3638\n",
            "Epoch [850/1000], Loss: 1204.3254\n",
            "Epoch [900/1000], Loss: 1204.2939\n",
            "Epoch [950/1000], Loss: 1204.2679\n",
            "Epoch [1000/1000], Loss: 1204.2467\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7-wQ_nA3eRF",
        "colab_type": "text"
      },
      "source": [
        "## Testing the Model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qZjRYQGe7r6P",
        "outputId": "e4f9d4db-08fe-4b46-a08a-4fbc9acf577a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "source": [
        "#test\n",
        "x_Py = torch.randn(50,1)\n",
        "x_Flat = torch.flatten(x_Py)\n",
        "x_list = np.abs(x_Flat.tolist())            # Convert to a list for ease of use, Take absolute value since we need stock prices > 0\n",
        "x_ = [x * 100 for x in x_list] \n",
        "\n",
        "y_ = gbm1.vector_bsm(option1, x_)\n",
        "\n",
        "x_Tensor =  torch.FloatTensor(x_)           # convert back to tensor\n",
        "x_RotTensor = x_Tensor.unsqueeze(1)\n",
        "\n",
        "y_Tensor =  torch.FloatTensor(y_)           # convert back to tensor\n",
        "y_RotTensor = y_Tensor.unsqueeze(1)\n",
        "#print(y_RotTensor)\n",
        "plt.scatter(x_RotTensor.detach().numpy(), y_RotTensor.detach().numpy(), label='true')\n",
        "\n",
        "y_pred = model(x_RotTensor)\n",
        "#print(y_pred)\n",
        "plt.scatter(x_RotTensor.detach().numpy(), y_pred.detach().numpy(), label='pred')\n",
        "\n",
        "plt.legend()"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f70db7764e0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 112
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD5CAYAAADcDXXiAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}