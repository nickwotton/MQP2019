{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BSM_NN_v04.ipynb ",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nickwotton/MQP2019/blob/master/Nick/BSM_NN_v04.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pu5uIyYP6Tnz",
        "colab_type": "text"
      },
      "source": [
        "# Attempt to Replicate the Black Scholes Model Using a Neural Network\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KOvg3FsM7r51",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.stats as ss\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4PCo06270zm",
        "colab_type": "text"
      },
      "source": [
        "## Define the Function\n",
        "Here we define our function, the Black Scholes Model (BSM). First, we must initialize the option class, then the Geometric Brownian Motion Class, and finally the BSM class.\n",
        "Then we test the equation with a test value of 2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wdyGtXXn7r56",
        "colab": {}
      },
      "source": [
        "'''=========\n",
        "option class init\n",
        "=========='''\n",
        "class VanillaOption:\n",
        "    def __init__(\n",
        "        self,\n",
        "        otype = 1, # 1: 'call'\n",
        "                  # -1: 'put'\n",
        "        strike = 110.,\n",
        "        maturity = 1.,\n",
        "        market_price = 10.):\n",
        "      self.otype = otype               # Put or Call\n",
        "      self.strike = strike             # Strike K\n",
        "      self.maturity = maturity         # Maturity T\n",
        "      self.market_price = market_price #this will be used for calibration\n",
        "      \n",
        "        \n",
        "    def payoff(self, s): #s: excercise price\n",
        "      otype = self.otype\n",
        "      k = self.strike\n",
        "      maturity = self.maturity\n",
        "      return np.max([0, (s - k)*otype])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ZT5HETX8jIs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''============\n",
        "Gbm class\n",
        "============='''\n",
        "\n",
        "class Gbm:\n",
        "    def __init__(self,\n",
        "                 init_state = 100.,\n",
        "                 drift_ratio = .0475,\n",
        "                 vol_ratio = .2\n",
        "                ):\n",
        "        self.init_state = init_state\n",
        "        self.drift_ratio = drift_ratio\n",
        "        self.vol_ratio = vol_ratio"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dkd7sAZCEYUk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''========\n",
        "Black-Scholes-Merton formula. \n",
        "=========='''\n",
        "\n",
        "def bsm_price(self, vanilla_option):\n",
        "    s0 = self.init_state\n",
        "    sigma = self.vol_ratio\n",
        "    r = self.drift_ratio\n",
        "    \n",
        "    otype = vanilla_option.otype\n",
        "    k = vanilla_option.strike\n",
        "    maturity = vanilla_option.maturity\n",
        "    \n",
        "    d1 = 1/(sigma*np.sqrt(maturity))*(np.log(s0/k) + (r + np.power(sigma,2)/2)*(maturity)) \n",
        "    d2 = 1/(sigma*np.sqrt(maturity))*(np.log(s0/k) + (r - np.power(sigma,2)/2)*(maturity)) \n",
        "    return (otype * s0 * ss.norm.cdf(otype * d1) #line break needs parenthesis\n",
        "            - otype * np.exp(-r * maturity) * k * ss.norm.cdf(otype * d2))\n",
        "\n",
        "Gbm.bsm_price = bsm_price"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vC1nkSnVIBgi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''=======\n",
        "Get BSM prices given an option and a Tensor\n",
        "======='''\n",
        "\n",
        "def prices_bsm(self, vanilla_option, data):\n",
        "  \n",
        "  # Create the list\n",
        "  a = []\n",
        "\n",
        "  # Get tensor size\n",
        "  sizeT = list(data.size())[0]\n",
        "\n",
        "  for i in range(sizeT):\n",
        "    self.init_state = data[i].item()\n",
        "    callPrice = gbm1.bsm_price(vanilla_option)\n",
        "    a.append(callPrice)\n",
        "  \n",
        "  # Convert to array then tensor\n",
        "  arrayOut = np.array(a)\n",
        "  outputData = torch.from_numpy(arrayOut)\n",
        "  return outputData\n",
        "\n",
        "Gbm.prices_bsm = prices_bsm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aoj7ImMqG5WG",
        "colab_type": "code",
        "outputId": "e6f47f00-1663-4e84-83f0-a102f59213ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "def f(s, k = 90):\n",
        "  gbm = Gbm(init_state=s)\n",
        "  option = VanillaOption(strike=k)\n",
        "  return gbm.bsm_price(option)\n",
        "\n",
        "batch_size = 63\n",
        "x_list = np.linspace(0, 200, batch_size)\n",
        "y_list = np.array([f(x) for x in x_list])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: RuntimeWarning: divide by zero encountered in log\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:15: RuntimeWarning: divide by zero encountered in log\n",
            "  from ipykernel import kernelapp as app\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7G7JVcog6Ref",
        "colab_type": "text"
      },
      "source": [
        "## Create Model\n",
        "Next, we create the neural network model. This is done first by setting the inner and outer dimensions with variables. Next we code the model and vary the internal dimensions to attempt to improve the model. At this level, this is essentially a simple linear algebra exercise:\n",
        "\n",
        "If we have input $x$, internal parameters $a,b$, and solution $f(x)$ then in the one-dimensional case we have:\n",
        "\n",
        "\\begin{equation}\n",
        "  \\left(\n",
        "    a_{1}x+b_{2}\n",
        "  \\right)\n",
        "  a_{2} + b_{2}\n",
        "  = f(x)\n",
        "\\end{equation}\n",
        "  \n",
        "However, we want to get a better estimate for the true equation. So we increase the interior dimension which corresponds to the number of neurons inside the network. For example, we raised the inner dimension to 3. In matrix form we have:\n",
        "\n",
        "\\begin{equation}\n",
        "\\left(\n",
        "  \\begin{bmatrix} x \\end{bmatrix} \n",
        "  \\begin{bmatrix} a_{1} & a_{2} & a_{3} \\end{bmatrix} \n",
        "  + \n",
        "  \\begin{bmatrix} b_{1} & b_{2} & b_{3} \\end{bmatrix}\n",
        "\\right)\n",
        " \\begin{bmatrix} a_{4} \\\\ a_{5} \\\\ a_{6} \\end{bmatrix}\n",
        " +\n",
        " \\begin{bmatrix} b_{4} \\\\ \\end{bmatrix}\n",
        " =\n",
        " \\begin{bmatrix} f(x) \\end{bmatrix}\n",
        "\\end{equation}\n",
        "\n",
        "Graphically, we can render this second neural network as:\n",
        "![Neural Network Diagram](https://drive.google.com/uc?id=1ItiBpdjPvWHF5ZWy8JDNDKq6dXfyU-IE)\n",
        "\n",
        "**ADD DESCRIPTION**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CiVUxudQ7r5_",
        "colab": {}
      },
      "source": [
        "#model\n",
        "#nn.Linear\n",
        "H1 = 80; H2 = 20 #number of hidden layer\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(1, H1), \n",
        "    nn.Sigmoid(),\n",
        "    nn.Linear(H1, H2),\n",
        "    nn.Sigmoid(),\n",
        "    nn.Linear(H2,2),\n",
        "    nn.Sigmoid(),\n",
        "    nn.Linear(2,1)\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkuS30xvzTgV",
        "colab_type": "text"
      },
      "source": [
        "Here we define the Loss function as the Mean Squared Error(MSE). \n",
        "\n",
        "Note that by doing so, we are essentially 'cheating' the system. In most applications, we would not know the function $f$ so we would be unable to find the MSE."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7TqjOaLs7r6C",
        "colab": {}
      },
      "source": [
        "#loss function \n",
        "criterion = nn.MSELoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIblNHIrzt8f",
        "colab_type": "text"
      },
      "source": [
        "Next we choose a learning rate and a method for learning. The learning rate is the percent of the data that is accepted in each iteration. The Methods we tried were SGD and Adam. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "E4rnJCtA7r6G",
        "colab": {}
      },
      "source": [
        "#optimizer\n",
        "learning_rate = 0.1\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.7) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3ez3tCHHSum",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = np.size(x_list)\n",
        "x_train0 = torch.from_numpy(x_list).reshape(batch_size,1).float()\n",
        "y_train0 = torch.from_numpy(y_list).reshape(batch_size,1).float()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-FHbIND2A34",
        "colab_type": "text"
      },
      "source": [
        "## Train the Model\n",
        "\n",
        "First we create the training data. This is a batch of random points that we pass through the BSM."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9VZMG4e2ziF",
        "colab_type": "text"
      },
      "source": [
        "Once we have the training data, we pass this collection of inputs and solutions into the model. With each iteration we calculate the loss and attempt to optimize the model to further reduce the loss.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EPsSIArQEgHR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Normalization\n",
        "\n",
        "def linear_transform(xx, l = 0, u= 1):\n",
        "  M = torch.max(xx)\n",
        "  m = torch.min(xx)\n",
        "  return (u-l)/(M-m)*(xx-m)+l, m, M, l, u\n",
        "x_train, x_m, x_M, x_l, x_u = linear_transform(x_train0, -1, 1)\n",
        "y_train, y_m, y_M, y_l, y_u = linear_transform(y_train0, 0, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4hNIFisK7r6M",
        "outputId": "4f7f1cdb-b859-417d-ccf1-94612755893b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Train the model\n",
        "\n",
        "num_epochs = 5000\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    # Forward pass\n",
        "    outputs = model(x_train.float())\n",
        "    loss = criterion(outputs, y_train.float())\n",
        "    \n",
        "    # Backward and optimize\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    if epoch == 0 or (epoch+1) % 50 == 0:\n",
        "        print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, \n",
        "                                                    num_epochs, loss.item()))\n",
        "        #print(outputs[1:10])\n",
        "       \n",
        "      \n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch [1/5000], Loss: 0.2129\n",
            "Epoch [50/5000], Loss: 0.1069\n",
            "Epoch [100/5000], Loss: 0.1040\n",
            "Epoch [150/5000], Loss: 0.0978\n",
            "Epoch [200/5000], Loss: 0.0823\n",
            "Epoch [250/5000], Loss: 0.0469\n",
            "Epoch [300/5000], Loss: 0.0165\n",
            "Epoch [350/5000], Loss: 0.0090\n",
            "Epoch [400/5000], Loss: 0.0059\n",
            "Epoch [450/5000], Loss: 0.0041\n",
            "Epoch [500/5000], Loss: 0.0030\n",
            "Epoch [550/5000], Loss: 0.0023\n",
            "Epoch [600/5000], Loss: 0.0018\n",
            "Epoch [650/5000], Loss: 0.0015\n",
            "Epoch [700/5000], Loss: 0.0013\n",
            "Epoch [750/5000], Loss: 0.0012\n",
            "Epoch [800/5000], Loss: 0.0011\n",
            "Epoch [850/5000], Loss: 0.0010\n",
            "Epoch [900/5000], Loss: 0.0009\n",
            "Epoch [950/5000], Loss: 0.0009\n",
            "Epoch [1000/5000], Loss: 0.0009\n",
            "Epoch [1050/5000], Loss: 0.0009\n",
            "Epoch [1100/5000], Loss: 0.0008\n",
            "Epoch [1150/5000], Loss: 0.0008\n",
            "Epoch [1200/5000], Loss: 0.0008\n",
            "Epoch [1250/5000], Loss: 0.0008\n",
            "Epoch [1300/5000], Loss: 0.0008\n",
            "Epoch [1350/5000], Loss: 0.0008\n",
            "Epoch [1400/5000], Loss: 0.0007\n",
            "Epoch [1450/5000], Loss: 0.0007\n",
            "Epoch [1500/5000], Loss: 0.0007\n",
            "Epoch [1550/5000], Loss: 0.0007\n",
            "Epoch [1600/5000], Loss: 0.0007\n",
            "Epoch [1650/5000], Loss: 0.0007\n",
            "Epoch [1700/5000], Loss: 0.0007\n",
            "Epoch [1750/5000], Loss: 0.0007\n",
            "Epoch [1800/5000], Loss: 0.0007\n",
            "Epoch [1850/5000], Loss: 0.0006\n",
            "Epoch [1900/5000], Loss: 0.0006\n",
            "Epoch [1950/5000], Loss: 0.0006\n",
            "Epoch [2000/5000], Loss: 0.0006\n",
            "Epoch [2050/5000], Loss: 0.0006\n",
            "Epoch [2100/5000], Loss: 0.0006\n",
            "Epoch [2150/5000], Loss: 0.0006\n",
            "Epoch [2200/5000], Loss: 0.0006\n",
            "Epoch [2250/5000], Loss: 0.0006\n",
            "Epoch [2300/5000], Loss: 0.0006\n",
            "Epoch [2350/5000], Loss: 0.0006\n",
            "Epoch [2400/5000], Loss: 0.0006\n",
            "Epoch [2450/5000], Loss: 0.0006\n",
            "Epoch [2500/5000], Loss: 0.0006\n",
            "Epoch [2550/5000], Loss: 0.0005\n",
            "Epoch [2600/5000], Loss: 0.0005\n",
            "Epoch [2650/5000], Loss: 0.0005\n",
            "Epoch [2700/5000], Loss: 0.0005\n",
            "Epoch [2750/5000], Loss: 0.0005\n",
            "Epoch [2800/5000], Loss: 0.0005\n",
            "Epoch [2850/5000], Loss: 0.0005\n",
            "Epoch [2900/5000], Loss: 0.0005\n",
            "Epoch [2950/5000], Loss: 0.0005\n",
            "Epoch [3000/5000], Loss: 0.0005\n",
            "Epoch [3050/5000], Loss: 0.0005\n",
            "Epoch [3100/5000], Loss: 0.0005\n",
            "Epoch [3150/5000], Loss: 0.0005\n",
            "Epoch [3200/5000], Loss: 0.0005\n",
            "Epoch [3250/5000], Loss: 0.0005\n",
            "Epoch [3300/5000], Loss: 0.0005\n",
            "Epoch [3350/5000], Loss: 0.0005\n",
            "Epoch [3400/5000], Loss: 0.0005\n",
            "Epoch [3450/5000], Loss: 0.0004\n",
            "Epoch [3500/5000], Loss: 0.0004\n",
            "Epoch [3550/5000], Loss: 0.0004\n",
            "Epoch [3600/5000], Loss: 0.0004\n",
            "Epoch [3650/5000], Loss: 0.0004\n",
            "Epoch [3700/5000], Loss: 0.0004\n",
            "Epoch [3750/5000], Loss: 0.0004\n",
            "Epoch [3800/5000], Loss: 0.0004\n",
            "Epoch [3850/5000], Loss: 0.0004\n",
            "Epoch [3900/5000], Loss: 0.0004\n",
            "Epoch [3950/5000], Loss: 0.0004\n",
            "Epoch [4000/5000], Loss: 0.0004\n",
            "Epoch [4050/5000], Loss: 0.0004\n",
            "Epoch [4100/5000], Loss: 0.0004\n",
            "Epoch [4150/5000], Loss: 0.0004\n",
            "Epoch [4200/5000], Loss: 0.0004\n",
            "Epoch [4250/5000], Loss: 0.0004\n",
            "Epoch [4300/5000], Loss: 0.0004\n",
            "Epoch [4350/5000], Loss: 0.0004\n",
            "Epoch [4400/5000], Loss: 0.0004\n",
            "Epoch [4450/5000], Loss: 0.0004\n",
            "Epoch [4500/5000], Loss: 0.0003\n",
            "Epoch [4550/5000], Loss: 0.0003\n",
            "Epoch [4600/5000], Loss: 0.0003\n",
            "Epoch [4650/5000], Loss: 0.0003\n",
            "Epoch [4700/5000], Loss: 0.0003\n",
            "Epoch [4750/5000], Loss: 0.0003\n",
            "Epoch [4800/5000], Loss: 0.0003\n",
            "Epoch [4850/5000], Loss: 0.0003\n",
            "Epoch [4900/5000], Loss: 0.0003\n",
            "Epoch [4950/5000], Loss: 0.0003\n",
            "Epoch [5000/5000], Loss: 0.0003\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7-wQ_nA3eRF",
        "colab_type": "text"
      },
      "source": [
        "## Testing the Model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JO-I6S5oFvzC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def learnedfun(x):\n",
        "  out = (1-(-1))/(x_M-x_m)*(x-x_m)+(-1.)\n",
        "  out = model(out)\n",
        "  out = (y_M- y_m)*out+y_m\n",
        "  return out\n",
        "\n",
        "y_pred = learnedfun(x_train0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "trvbzo52HoLF",
        "colab_type": "code",
        "outputId": "0dfd3ae8-faef-454c-ad21-03b0ae19d4e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267
        }
      },
      "source": [
        "# Test with training data\n",
        "plt.scatter(x_train0.detach().numpy(), y_train0.detach().numpy(), label='true')\n",
        "plt.scatter(x_train0.detach().numpy(), y_pred.detach().numpy(), label='pred')\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD6CAYAAABamQdMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nXzPC66TPAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qZjRYQGe7r6P",
        "outputId": "ba7e97a0-a0f7-4234-b735-52a954ddc618",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "source": [
        "# Test the Model with Random data\n",
        "\n",
        "# Generate random data\n",
        "x_0 = np.random.randint(0, 200, batch_size) # MIGHT HAVE TO DO WITH THIS RANGE AND BATCH SIZE?\n",
        "\n",
        "# Get BSM Prices as determined by the formula\n",
        "y_0 = np.array([f(x) for x in x_0])\n",
        "\n",
        "# Transform to tensor\n",
        "x_0 = torch.from_numpy(x_0).reshape(batch_size,1).float()\n",
        "y_0 = torch.from_numpy(y_0).reshape(batch_size,1).float()\n",
        "\n",
        "# Normalize\n",
        "x_, x_m, x_M, x_l, x_u = linear_transform(x_0)\n",
        "y_, y_m, y_M, y_l, y_u = linear_transform(y_0)\n",
        "\n",
        "# Plot x_ versus formula prices\n",
        "plt.scatter(x_0.detach().numpy(), y_0.detach().numpy(), label='true')\n",
        "\n",
        "# Get BSM Prices as determined by the model\n",
        "y_pred = learnedfun(x_0)\n",
        "\n",
        "# Plot x_ versus the model prices\n",
        "plt.scatter(x_0.detach().numpy(), y_pred.detach().numpy(), label='pred')\n",
        "\n",
        "plt.legend()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7ff3960a1208>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 306
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdZRoBeC3YKM",
        "colab_type": "text"
      },
      "source": [
        "Put the above into a loop and check each number of neurons and layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "btJeLUznONYl",
        "colab_type": "code",
        "outputId": "b554740f-7c8a-49ba-94d7-e0616f6bd105",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Begin loop through each\n",
        "# TODO Add time Checking\n",
        "neuronsList = [[50,12],\n",
        "               [50,12,10],\n",
        "               [50,30],\n",
        "               [50,50],\n",
        "               [50,50,21],\n",
        "               [50,100],\n",
        "               [80,20],\n",
        "               [100,80],\n",
        "               [100,100],\n",
        "               [100,120],\n",
        "               [100,150],\n",
        "               [500,100]]\n",
        "for numNeurons in neuronsList:\n",
        "  # Define the Model\n",
        "  if len(numNeurons) == 2:\n",
        "    H1 = numNeurons[0]; H2 = numNeurons[1] #number of hidden layer neurons\n",
        "    model = nn.Sequential(\n",
        "      nn.Linear(1, H1), \n",
        "      nn.Sigmoid(),\n",
        "      nn.Linear(H1, H2),\n",
        "      nn.Sigmoid(),\n",
        "      nn.Linear(H2,2),\n",
        "      nn.Sigmoid(),\n",
        "      nn.Linear(2,1))\n",
        "  else:\n",
        "    H1 = numNeurons[0]; H2 = numNeurons[1]; H3 = numNeurons[2] #number of hidden layer neurons\n",
        "    model = nn.Sequential(\n",
        "      nn.Linear(1, H1), \n",
        "      nn.Sigmoid(),\n",
        "      nn.Linear(H1, H2),\n",
        "      nn.Sigmoid(),\n",
        "      nn.Linear(H2,H3),\n",
        "      nn.Sigmoid(),\n",
        "      nn.Linear(H3,2),\n",
        "      nn.Sigmoid(),\n",
        "      nn.Linear(2,1))\n",
        "\n",
        "  #loss function \n",
        "  criterion = nn.MSELoss()\n",
        "\n",
        "  #optimizer\n",
        "  learning_rate = 0.1\n",
        "  optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.7) \n",
        "\n",
        "  batch_size = np.size(x_list)\n",
        "  x_train0 = torch.from_numpy(x_list).reshape(batch_size,1).float()\n",
        "  y_train0 = torch.from_numpy(y_list).reshape(batch_size,1).float()\n",
        "\n",
        "  # Normalization\n",
        "  x_train, x_m, x_M, x_l, x_u = linear_transform(x_train0, -1, 1)\n",
        "  y_train, y_m, y_M, y_l, y_u = linear_transform(y_train0, 0, 1)\n",
        "\n",
        "  # Train the model\n",
        "\n",
        "  print(\"Model: \" + str(numNeurons))\n",
        "  start = time.time()\n",
        "  num_epochs = 5000\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "\n",
        "      # Forward pass\n",
        "      outputs = model(x_train.float())\n",
        "      loss = criterion(outputs, y_train.float())\n",
        "    \n",
        "      # Backward and optimize\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "    \n",
        "      if epoch == 0 or (epoch+1) % 50 == 0:\n",
        "          print ('Epoch [{}/{}], Loss: {:.8f}'.format(epoch+1, \n",
        "                                                      num_epochs, loss.item()))\n",
        "          #print(outputs[1:10])\n",
        "  end = time.time()\n",
        "  timeEl = end-start\n",
        "  print(\"Total Time Elapsed (s): \" + str(timeEl))\n",
        "  print(\"\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: [50, 12]\n",
            "Epoch [1/5000], Loss: 0.33672205\n",
            "Epoch [50/5000], Loss: 0.10862195\n",
            "Epoch [100/5000], Loss: 0.10738762\n",
            "Epoch [150/5000], Loss: 0.10563789\n",
            "Epoch [200/5000], Loss: 0.10259616\n",
            "Epoch [250/5000], Loss: 0.09643578\n",
            "Epoch [300/5000], Loss: 0.08241504\n",
            "Epoch [350/5000], Loss: 0.05174055\n",
            "Epoch [400/5000], Loss: 0.01728370\n",
            "Epoch [450/5000], Loss: 0.00771000\n",
            "Epoch [500/5000], Loss: 0.00525315\n",
            "Epoch [550/5000], Loss: 0.00392157\n",
            "Epoch [600/5000], Loss: 0.00308993\n",
            "Epoch [650/5000], Loss: 0.00253730\n",
            "Epoch [700/5000], Loss: 0.00215737\n",
            "Epoch [750/5000], Loss: 0.00189060\n",
            "Epoch [800/5000], Loss: 0.00170050\n",
            "Epoch [850/5000], Loss: 0.00156336\n",
            "Epoch [900/5000], Loss: 0.00146328\n",
            "Epoch [950/5000], Loss: 0.00138933\n",
            "Epoch [1000/5000], Loss: 0.00133389\n",
            "Epoch [1050/5000], Loss: 0.00129160\n",
            "Epoch [1100/5000], Loss: 0.00125865\n",
            "Epoch [1150/5000], Loss: 0.00123235\n",
            "Epoch [1200/5000], Loss: 0.00121077\n",
            "Epoch [1250/5000], Loss: 0.00119254\n",
            "Epoch [1300/5000], Loss: 0.00117669\n",
            "Epoch [1350/5000], Loss: 0.00116252\n",
            "Epoch [1400/5000], Loss: 0.00114955\n",
            "Epoch [1450/5000], Loss: 0.00113742\n",
            "Epoch [1500/5000], Loss: 0.00112590\n",
            "Epoch [1550/5000], Loss: 0.00111483\n",
            "Epoch [1600/5000], Loss: 0.00110409\n",
            "Epoch [1650/5000], Loss: 0.00109360\n",
            "Epoch [1700/5000], Loss: 0.00108331\n",
            "Epoch [1750/5000], Loss: 0.00107319\n",
            "Epoch [1800/5000], Loss: 0.00106321\n",
            "Epoch [1850/5000], Loss: 0.00105336\n",
            "Epoch [1900/5000], Loss: 0.00104363\n",
            "Epoch [1950/5000], Loss: 0.00103402\n",
            "Epoch [2000/5000], Loss: 0.00102451\n",
            "Epoch [2050/5000], Loss: 0.00101510\n",
            "Epoch [2100/5000], Loss: 0.00100579\n",
            "Epoch [2150/5000], Loss: 0.00099657\n",
            "Epoch [2200/5000], Loss: 0.00098744\n",
            "Epoch [2250/5000], Loss: 0.00097840\n",
            "Epoch [2300/5000], Loss: 0.00096944\n",
            "Epoch [2350/5000], Loss: 0.00096056\n",
            "Epoch [2400/5000], Loss: 0.00095175\n",
            "Epoch [2450/5000], Loss: 0.00094302\n",
            "Epoch [2500/5000], Loss: 0.00093436\n",
            "Epoch [2550/5000], Loss: 0.00092577\n",
            "Epoch [2600/5000], Loss: 0.00091724\n",
            "Epoch [2650/5000], Loss: 0.00090878\n",
            "Epoch [2700/5000], Loss: 0.00090038\n",
            "Epoch [2750/5000], Loss: 0.00089204\n",
            "Epoch [2800/5000], Loss: 0.00088376\n",
            "Epoch [2850/5000], Loss: 0.00087553\n",
            "Epoch [2900/5000], Loss: 0.00086737\n",
            "Epoch [2950/5000], Loss: 0.00085926\n",
            "Epoch [3000/5000], Loss: 0.00085120\n",
            "Epoch [3050/5000], Loss: 0.00084320\n",
            "Epoch [3100/5000], Loss: 0.00083526\n",
            "Epoch [3150/5000], Loss: 0.00082737\n",
            "Epoch [3200/5000], Loss: 0.00081953\n",
            "Epoch [3250/5000], Loss: 0.00081174\n",
            "Epoch [3300/5000], Loss: 0.00080401\n",
            "Epoch [3350/5000], Loss: 0.00079633\n",
            "Epoch [3400/5000], Loss: 0.00078871\n",
            "Epoch [3450/5000], Loss: 0.00078114\n",
            "Epoch [3500/5000], Loss: 0.00077362\n",
            "Epoch [3550/5000], Loss: 0.00076616\n",
            "Epoch [3600/5000], Loss: 0.00075875\n",
            "Epoch [3650/5000], Loss: 0.00075139\n",
            "Epoch [3700/5000], Loss: 0.00074409\n",
            "Epoch [3750/5000], Loss: 0.00073685\n",
            "Epoch [3800/5000], Loss: 0.00072966\n",
            "Epoch [3850/5000], Loss: 0.00072252\n",
            "Epoch [3900/5000], Loss: 0.00071545\n",
            "Epoch [3950/5000], Loss: 0.00070842\n",
            "Epoch [4000/5000], Loss: 0.00070146\n",
            "Epoch [4050/5000], Loss: 0.00069455\n",
            "Epoch [4100/5000], Loss: 0.00068770\n",
            "Epoch [4150/5000], Loss: 0.00068091\n",
            "Epoch [4200/5000], Loss: 0.00067417\n",
            "Epoch [4250/5000], Loss: 0.00066749\n",
            "Epoch [4300/5000], Loss: 0.00066088\n",
            "Epoch [4350/5000], Loss: 0.00065432\n",
            "Epoch [4400/5000], Loss: 0.00064782\n",
            "Epoch [4450/5000], Loss: 0.00064137\n",
            "Epoch [4500/5000], Loss: 0.00063499\n",
            "Epoch [4550/5000], Loss: 0.00062867\n",
            "Epoch [4600/5000], Loss: 0.00062241\n",
            "Epoch [4650/5000], Loss: 0.00061621\n",
            "Epoch [4700/5000], Loss: 0.00061006\n",
            "Epoch [4750/5000], Loss: 0.00060398\n",
            "Epoch [4800/5000], Loss: 0.00059796\n",
            "Epoch [4850/5000], Loss: 0.00059200\n",
            "Epoch [4900/5000], Loss: 0.00058610\n",
            "Epoch [4950/5000], Loss: 0.00058026\n",
            "Epoch [5000/5000], Loss: 0.00057448\n",
            "Total Time Elapsed (s): 4.441021919250488\n",
            "\n",
            "Model: [50, 12, 10]\n",
            "Epoch [1/5000], Loss: 0.16300529\n",
            "Epoch [50/5000], Loss: 0.10887753\n",
            "Epoch [100/5000], Loss: 0.10883829\n",
            "Epoch [150/5000], Loss: 0.10879560\n",
            "Epoch [200/5000], Loss: 0.10874850\n",
            "Epoch [250/5000], Loss: 0.10869586\n",
            "Epoch [300/5000], Loss: 0.10863618\n",
            "Epoch [350/5000], Loss: 0.10856763\n",
            "Epoch [400/5000], Loss: 0.10848791\n",
            "Epoch [450/5000], Loss: 0.10839393\n",
            "Epoch [500/5000], Loss: 0.10828161\n",
            "Epoch [550/5000], Loss: 0.10814541\n",
            "Epoch [600/5000], Loss: 0.10797762\n",
            "Epoch [650/5000], Loss: 0.10776732\n",
            "Epoch [700/5000], Loss: 0.10749846\n",
            "Epoch [750/5000], Loss: 0.10714704\n",
            "Epoch [800/5000], Loss: 0.10667550\n",
            "Epoch [850/5000], Loss: 0.10602285\n",
            "Epoch [900/5000], Loss: 0.10508500\n",
            "Epoch [950/5000], Loss: 0.10367378\n",
            "Epoch [1000/5000], Loss: 0.10142446\n",
            "Epoch [1050/5000], Loss: 0.09757282\n",
            "Epoch [1100/5000], Loss: 0.09039161\n",
            "Epoch [1150/5000], Loss: 0.07595895\n",
            "Epoch [1200/5000], Loss: 0.04876474\n",
            "Epoch [1250/5000], Loss: 0.02047332\n",
            "Epoch [1300/5000], Loss: 0.01075574\n",
            "Epoch [1350/5000], Loss: 0.00730362\n",
            "Epoch [1400/5000], Loss: 0.00536273\n",
            "Epoch [1450/5000], Loss: 0.00412358\n",
            "Epoch [1500/5000], Loss: 0.00328189\n",
            "Epoch [1550/5000], Loss: 0.00268961\n",
            "Epoch [1600/5000], Loss: 0.00226343\n",
            "Epoch [1650/5000], Loss: 0.00195198\n",
            "Epoch [1700/5000], Loss: 0.00172164\n",
            "Epoch [1750/5000], Loss: 0.00154956\n",
            "Epoch [1800/5000], Loss: 0.00141974\n",
            "Epoch [1850/5000], Loss: 0.00132080\n",
            "Epoch [1900/5000], Loss: 0.00124453\n",
            "Epoch [1950/5000], Loss: 0.00118494\n",
            "Epoch [2000/5000], Loss: 0.00113766\n",
            "Epoch [2050/5000], Loss: 0.00109948\n",
            "Epoch [2100/5000], Loss: 0.00106802\n",
            "Epoch [2150/5000], Loss: 0.00104154\n",
            "Epoch [2200/5000], Loss: 0.00101875\n",
            "Epoch [2250/5000], Loss: 0.00099873\n",
            "Epoch [2300/5000], Loss: 0.00098078\n",
            "Epoch [2350/5000], Loss: 0.00096440\n",
            "Epoch [2400/5000], Loss: 0.00094923\n",
            "Epoch [2450/5000], Loss: 0.00093502\n",
            "Epoch [2500/5000], Loss: 0.00092158\n",
            "Epoch [2550/5000], Loss: 0.00090879\n",
            "Epoch [2600/5000], Loss: 0.00089654\n",
            "Epoch [2650/5000], Loss: 0.00088477\n",
            "Epoch [2700/5000], Loss: 0.00087343\n",
            "Epoch [2750/5000], Loss: 0.00086247\n",
            "Epoch [2800/5000], Loss: 0.00085188\n",
            "Epoch [2850/5000], Loss: 0.00084161\n",
            "Epoch [2900/5000], Loss: 0.00083165\n",
            "Epoch [2950/5000], Loss: 0.00082198\n",
            "Epoch [3000/5000], Loss: 0.00081257\n",
            "Epoch [3050/5000], Loss: 0.00080342\n",
            "Epoch [3100/5000], Loss: 0.00079450\n",
            "Epoch [3150/5000], Loss: 0.00078580\n",
            "Epoch [3200/5000], Loss: 0.00077732\n",
            "Epoch [3250/5000], Loss: 0.00076904\n",
            "Epoch [3300/5000], Loss: 0.00076093\n",
            "Epoch [3350/5000], Loss: 0.00075301\n",
            "Epoch [3400/5000], Loss: 0.00074526\n",
            "Epoch [3450/5000], Loss: 0.00073767\n",
            "Epoch [3500/5000], Loss: 0.00073023\n",
            "Epoch [3550/5000], Loss: 0.00072293\n",
            "Epoch [3600/5000], Loss: 0.00071578\n",
            "Epoch [3650/5000], Loss: 0.00070875\n",
            "Epoch [3700/5000], Loss: 0.00070186\n",
            "Epoch [3750/5000], Loss: 0.00069509\n",
            "Epoch [3800/5000], Loss: 0.00068844\n",
            "Epoch [3850/5000], Loss: 0.00068191\n",
            "Epoch [3900/5000], Loss: 0.00067548\n",
            "Epoch [3950/5000], Loss: 0.00066917\n",
            "Epoch [4000/5000], Loss: 0.00066295\n",
            "Epoch [4050/5000], Loss: 0.00065684\n",
            "Epoch [4100/5000], Loss: 0.00065083\n",
            "Epoch [4150/5000], Loss: 0.00064491\n",
            "Epoch [4200/5000], Loss: 0.00063909\n",
            "Epoch [4250/5000], Loss: 0.00063336\n",
            "Epoch [4300/5000], Loss: 0.00062771\n",
            "Epoch [4350/5000], Loss: 0.00062216\n",
            "Epoch [4400/5000], Loss: 0.00061668\n",
            "Epoch [4450/5000], Loss: 0.00061129\n",
            "Epoch [4500/5000], Loss: 0.00060598\n",
            "Epoch [4550/5000], Loss: 0.00060075\n",
            "Epoch [4600/5000], Loss: 0.00059560\n",
            "Epoch [4650/5000], Loss: 0.00059053\n",
            "Epoch [4700/5000], Loss: 0.00058553\n",
            "Epoch [4750/5000], Loss: 0.00058061\n",
            "Epoch [4800/5000], Loss: 0.00057575\n",
            "Epoch [4850/5000], Loss: 0.00057097\n",
            "Epoch [4900/5000], Loss: 0.00056626\n",
            "Epoch [4950/5000], Loss: 0.00056162\n",
            "Epoch [5000/5000], Loss: 0.00055705\n",
            "Total Time Elapsed (s): 5.121997117996216\n",
            "\n",
            "Model: [50, 30]\n",
            "Epoch [1/5000], Loss: 0.62863886\n",
            "Epoch [50/5000], Loss: 0.10792189\n",
            "Epoch [100/5000], Loss: 0.10681345\n",
            "Epoch [150/5000], Loss: 0.10490339\n",
            "Epoch [200/5000], Loss: 0.10100080\n",
            "Epoch [250/5000], Loss: 0.09161557\n",
            "Epoch [300/5000], Loss: 0.06696136\n",
            "Epoch [350/5000], Loss: 0.02526072\n",
            "Epoch [400/5000], Loss: 0.01159334\n",
            "Epoch [450/5000], Loss: 0.00950693\n",
            "Epoch [500/5000], Loss: 0.00805856\n",
            "Epoch [550/5000], Loss: 0.00690484\n",
            "Epoch [600/5000], Loss: 0.00595400\n",
            "Epoch [650/5000], Loss: 0.00515692\n",
            "Epoch [700/5000], Loss: 0.00448502\n",
            "Epoch [750/5000], Loss: 0.00391912\n",
            "Epoch [800/5000], Loss: 0.00344442\n",
            "Epoch [850/5000], Loss: 0.00304840\n",
            "Epoch [900/5000], Loss: 0.00271996\n",
            "Epoch [950/5000], Loss: 0.00244908\n",
            "Epoch [1000/5000], Loss: 0.00222680\n",
            "Epoch [1050/5000], Loss: 0.00204513\n",
            "Epoch [1100/5000], Loss: 0.00189711\n",
            "Epoch [1150/5000], Loss: 0.00177674\n",
            "Epoch [1200/5000], Loss: 0.00167889\n",
            "Epoch [1250/5000], Loss: 0.00159928\n",
            "Epoch [1300/5000], Loss: 0.00153432\n",
            "Epoch [1350/5000], Loss: 0.00148107\n",
            "Epoch [1400/5000], Loss: 0.00143713\n",
            "Epoch [1450/5000], Loss: 0.00140053\n",
            "Epoch [1500/5000], Loss: 0.00136973\n",
            "Epoch [1550/5000], Loss: 0.00134346\n",
            "Epoch [1600/5000], Loss: 0.00132074\n",
            "Epoch [1650/5000], Loss: 0.00130077\n",
            "Epoch [1700/5000], Loss: 0.00128295\n",
            "Epoch [1750/5000], Loss: 0.00126680\n",
            "Epoch [1800/5000], Loss: 0.00125194\n",
            "Epoch [1850/5000], Loss: 0.00123810\n",
            "Epoch [1900/5000], Loss: 0.00122507\n",
            "Epoch [1950/5000], Loss: 0.00121268\n",
            "Epoch [2000/5000], Loss: 0.00120081\n",
            "Epoch [2050/5000], Loss: 0.00118938\n",
            "Epoch [2100/5000], Loss: 0.00117831\n",
            "Epoch [2150/5000], Loss: 0.00116757\n",
            "Epoch [2200/5000], Loss: 0.00115711\n",
            "Epoch [2250/5000], Loss: 0.00114691\n",
            "Epoch [2300/5000], Loss: 0.00113694\n",
            "Epoch [2350/5000], Loss: 0.00112719\n",
            "Epoch [2400/5000], Loss: 0.00111765\n",
            "Epoch [2450/5000], Loss: 0.00110829\n",
            "Epoch [2500/5000], Loss: 0.00109911\n",
            "Epoch [2550/5000], Loss: 0.00109010\n",
            "Epoch [2600/5000], Loss: 0.00108124\n",
            "Epoch [2650/5000], Loss: 0.00107253\n",
            "Epoch [2700/5000], Loss: 0.00106395\n",
            "Epoch [2750/5000], Loss: 0.00105550\n",
            "Epoch [2800/5000], Loss: 0.00104716\n",
            "Epoch [2850/5000], Loss: 0.00103893\n",
            "Epoch [2900/5000], Loss: 0.00103080\n",
            "Epoch [2950/5000], Loss: 0.00102276\n",
            "Epoch [3000/5000], Loss: 0.00101481\n",
            "Epoch [3050/5000], Loss: 0.00100693\n",
            "Epoch [3100/5000], Loss: 0.00099913\n",
            "Epoch [3150/5000], Loss: 0.00099139\n",
            "Epoch [3200/5000], Loss: 0.00098372\n",
            "Epoch [3250/5000], Loss: 0.00097610\n",
            "Epoch [3300/5000], Loss: 0.00096852\n",
            "Epoch [3350/5000], Loss: 0.00096100\n",
            "Epoch [3400/5000], Loss: 0.00095352\n",
            "Epoch [3450/5000], Loss: 0.00094608\n",
            "Epoch [3500/5000], Loss: 0.00093868\n",
            "Epoch [3550/5000], Loss: 0.00093131\n",
            "Epoch [3600/5000], Loss: 0.00092397\n",
            "Epoch [3650/5000], Loss: 0.00091665\n",
            "Epoch [3700/5000], Loss: 0.00090936\n",
            "Epoch [3750/5000], Loss: 0.00090210\n",
            "Epoch [3800/5000], Loss: 0.00089485\n",
            "Epoch [3850/5000], Loss: 0.00088763\n",
            "Epoch [3900/5000], Loss: 0.00088043\n",
            "Epoch [3950/5000], Loss: 0.00087324\n",
            "Epoch [4000/5000], Loss: 0.00086607\n",
            "Epoch [4050/5000], Loss: 0.00085891\n",
            "Epoch [4100/5000], Loss: 0.00085177\n",
            "Epoch [4150/5000], Loss: 0.00084465\n",
            "Epoch [4200/5000], Loss: 0.00083754\n",
            "Epoch [4250/5000], Loss: 0.00083044\n",
            "Epoch [4300/5000], Loss: 0.00082335\n",
            "Epoch [4350/5000], Loss: 0.00081628\n",
            "Epoch [4400/5000], Loss: 0.00080922\n",
            "Epoch [4450/5000], Loss: 0.00080217\n",
            "Epoch [4500/5000], Loss: 0.00079514\n",
            "Epoch [4550/5000], Loss: 0.00078812\n",
            "Epoch [4600/5000], Loss: 0.00078111\n",
            "Epoch [4650/5000], Loss: 0.00077412\n",
            "Epoch [4700/5000], Loss: 0.00076715\n",
            "Epoch [4750/5000], Loss: 0.00076019\n",
            "Epoch [4800/5000], Loss: 0.00075324\n",
            "Epoch [4850/5000], Loss: 0.00074632\n",
            "Epoch [4900/5000], Loss: 0.00073941\n",
            "Epoch [4950/5000], Loss: 0.00073252\n",
            "Epoch [5000/5000], Loss: 0.00072565\n",
            "Total Time Elapsed (s): 4.549135208129883\n",
            "\n",
            "Model: [50, 50]\n",
            "Epoch [1/5000], Loss: 0.11309291\n",
            "Epoch [50/5000], Loss: 0.10730787\n",
            "Epoch [100/5000], Loss: 0.10465167\n",
            "Epoch [150/5000], Loss: 0.09938221\n",
            "Epoch [200/5000], Loss: 0.08650237\n",
            "Epoch [250/5000], Loss: 0.05471025\n",
            "Epoch [300/5000], Loss: 0.01761661\n",
            "Epoch [350/5000], Loss: 0.00896102\n",
            "Epoch [400/5000], Loss: 0.00628320\n",
            "Epoch [450/5000], Loss: 0.00464596\n",
            "Epoch [500/5000], Loss: 0.00354725\n",
            "Epoch [550/5000], Loss: 0.00278774\n",
            "Epoch [600/5000], Loss: 0.00225800\n",
            "Epoch [650/5000], Loss: 0.00188790\n",
            "Epoch [700/5000], Loss: 0.00162941\n",
            "Epoch [750/5000], Loss: 0.00144886\n",
            "Epoch [800/5000], Loss: 0.00132254\n",
            "Epoch [850/5000], Loss: 0.00123375\n",
            "Epoch [900/5000], Loss: 0.00117085\n",
            "Epoch [950/5000], Loss: 0.00112572\n",
            "Epoch [1000/5000], Loss: 0.00109274\n",
            "Epoch [1050/5000], Loss: 0.00106804\n",
            "Epoch [1100/5000], Loss: 0.00104898\n",
            "Epoch [1150/5000], Loss: 0.00103372\n",
            "Epoch [1200/5000], Loss: 0.00102105\n",
            "Epoch [1250/5000], Loss: 0.00101012\n",
            "Epoch [1300/5000], Loss: 0.00100037\n",
            "Epoch [1350/5000], Loss: 0.00099141\n",
            "Epoch [1400/5000], Loss: 0.00098299\n",
            "Epoch [1450/5000], Loss: 0.00097493\n",
            "Epoch [1500/5000], Loss: 0.00096713\n",
            "Epoch [1550/5000], Loss: 0.00095951\n",
            "Epoch [1600/5000], Loss: 0.00095202\n",
            "Epoch [1650/5000], Loss: 0.00094463\n",
            "Epoch [1700/5000], Loss: 0.00093732\n",
            "Epoch [1750/5000], Loss: 0.00093008\n",
            "Epoch [1800/5000], Loss: 0.00092290\n",
            "Epoch [1850/5000], Loss: 0.00091577\n",
            "Epoch [1900/5000], Loss: 0.00090870\n",
            "Epoch [1950/5000], Loss: 0.00090168\n",
            "Epoch [2000/5000], Loss: 0.00089472\n",
            "Epoch [2050/5000], Loss: 0.00088780\n",
            "Epoch [2100/5000], Loss: 0.00088094\n",
            "Epoch [2150/5000], Loss: 0.00087413\n",
            "Epoch [2200/5000], Loss: 0.00086737\n",
            "Epoch [2250/5000], Loss: 0.00086066\n",
            "Epoch [2300/5000], Loss: 0.00085401\n",
            "Epoch [2350/5000], Loss: 0.00084740\n",
            "Epoch [2400/5000], Loss: 0.00084085\n",
            "Epoch [2450/5000], Loss: 0.00083434\n",
            "Epoch [2500/5000], Loss: 0.00082789\n",
            "Epoch [2550/5000], Loss: 0.00082149\n",
            "Epoch [2600/5000], Loss: 0.00081514\n",
            "Epoch [2650/5000], Loss: 0.00080883\n",
            "Epoch [2700/5000], Loss: 0.00080257\n",
            "Epoch [2750/5000], Loss: 0.00079637\n",
            "Epoch [2800/5000], Loss: 0.00079020\n",
            "Epoch [2850/5000], Loss: 0.00078408\n",
            "Epoch [2900/5000], Loss: 0.00077801\n",
            "Epoch [2950/5000], Loss: 0.00077199\n",
            "Epoch [3000/5000], Loss: 0.00076600\n",
            "Epoch [3050/5000], Loss: 0.00076007\n",
            "Epoch [3100/5000], Loss: 0.00075417\n",
            "Epoch [3150/5000], Loss: 0.00074832\n",
            "Epoch [3200/5000], Loss: 0.00074250\n",
            "Epoch [3250/5000], Loss: 0.00073673\n",
            "Epoch [3300/5000], Loss: 0.00073100\n",
            "Epoch [3350/5000], Loss: 0.00072530\n",
            "Epoch [3400/5000], Loss: 0.00071965\n",
            "Epoch [3450/5000], Loss: 0.00071403\n",
            "Epoch [3500/5000], Loss: 0.00070845\n",
            "Epoch [3550/5000], Loss: 0.00070291\n",
            "Epoch [3600/5000], Loss: 0.00069741\n",
            "Epoch [3650/5000], Loss: 0.00069194\n",
            "Epoch [3700/5000], Loss: 0.00068650\n",
            "Epoch [3750/5000], Loss: 0.00068110\n",
            "Epoch [3800/5000], Loss: 0.00067574\n",
            "Epoch [3850/5000], Loss: 0.00067041\n",
            "Epoch [3900/5000], Loss: 0.00066511\n",
            "Epoch [3950/5000], Loss: 0.00065984\n",
            "Epoch [4000/5000], Loss: 0.00065461\n",
            "Epoch [4050/5000], Loss: 0.00064941\n",
            "Epoch [4100/5000], Loss: 0.00064424\n",
            "Epoch [4150/5000], Loss: 0.00063911\n",
            "Epoch [4200/5000], Loss: 0.00063400\n",
            "Epoch [4250/5000], Loss: 0.00062893\n",
            "Epoch [4300/5000], Loss: 0.00062388\n",
            "Epoch [4350/5000], Loss: 0.00061887\n",
            "Epoch [4400/5000], Loss: 0.00061388\n",
            "Epoch [4450/5000], Loss: 0.00060893\n",
            "Epoch [4500/5000], Loss: 0.00060401\n",
            "Epoch [4550/5000], Loss: 0.00059911\n",
            "Epoch [4600/5000], Loss: 0.00059424\n",
            "Epoch [4650/5000], Loss: 0.00058941\n",
            "Epoch [4700/5000], Loss: 0.00058460\n",
            "Epoch [4750/5000], Loss: 0.00057981\n",
            "Epoch [4800/5000], Loss: 0.00057506\n",
            "Epoch [4850/5000], Loss: 0.00057034\n",
            "Epoch [4900/5000], Loss: 0.00056564\n",
            "Epoch [4950/5000], Loss: 0.00056097\n",
            "Epoch [5000/5000], Loss: 0.00055633\n",
            "Total Time Elapsed (s): 4.464671850204468\n",
            "\n",
            "Model: [50, 50, 21]\n",
            "Epoch [1/5000], Loss: 0.44244829\n",
            "Epoch [50/5000], Loss: 0.10902856\n",
            "Epoch [100/5000], Loss: 0.10901625\n",
            "Epoch [150/5000], Loss: 0.10900392\n",
            "Epoch [200/5000], Loss: 0.10899136\n",
            "Epoch [250/5000], Loss: 0.10897847\n",
            "Epoch [300/5000], Loss: 0.10896508\n",
            "Epoch [350/5000], Loss: 0.10895099\n",
            "Epoch [400/5000], Loss: 0.10893600\n",
            "Epoch [450/5000], Loss: 0.10891990\n",
            "Epoch [500/5000], Loss: 0.10890245\n",
            "Epoch [550/5000], Loss: 0.10888338\n",
            "Epoch [600/5000], Loss: 0.10886230\n",
            "Epoch [650/5000], Loss: 0.10883886\n",
            "Epoch [700/5000], Loss: 0.10881259\n",
            "Epoch [750/5000], Loss: 0.10878289\n",
            "Epoch [800/5000], Loss: 0.10874906\n",
            "Epoch [850/5000], Loss: 0.10871022\n",
            "Epoch [900/5000], Loss: 0.10866527\n",
            "Epoch [950/5000], Loss: 0.10861281\n",
            "Epoch [1000/5000], Loss: 0.10855100\n",
            "Epoch [1050/5000], Loss: 0.10847746\n",
            "Epoch [1100/5000], Loss: 0.10838903\n",
            "Epoch [1150/5000], Loss: 0.10828140\n",
            "Epoch [1200/5000], Loss: 0.10814855\n",
            "Epoch [1250/5000], Loss: 0.10798204\n",
            "Epoch [1300/5000], Loss: 0.10776953\n",
            "Epoch [1350/5000], Loss: 0.10749252\n",
            "Epoch [1400/5000], Loss: 0.10712235\n",
            "Epoch [1450/5000], Loss: 0.10661265\n",
            "Epoch [1500/5000], Loss: 0.10588472\n",
            "Epoch [1550/5000], Loss: 0.10479687\n",
            "Epoch [1600/5000], Loss: 0.10307554\n",
            "Epoch [1650/5000], Loss: 0.10014495\n",
            "Epoch [1700/5000], Loss: 0.09466598\n",
            "Epoch [1750/5000], Loss: 0.08323690\n",
            "Epoch [1800/5000], Loss: 0.05812399\n",
            "Epoch [1850/5000], Loss: 0.02197139\n",
            "Epoch [1900/5000], Loss: 0.01048546\n",
            "Epoch [1950/5000], Loss: 0.00847082\n",
            "Epoch [2000/5000], Loss: 0.00710294\n",
            "Epoch [2050/5000], Loss: 0.00606098\n",
            "Epoch [2100/5000], Loss: 0.00523985\n",
            "Epoch [2150/5000], Loss: 0.00457759\n",
            "Epoch [2200/5000], Loss: 0.00403556\n",
            "Epoch [2250/5000], Loss: 0.00358793\n",
            "Epoch [2300/5000], Loss: 0.00321632\n",
            "Epoch [2350/5000], Loss: 0.00290701\n",
            "Epoch [2400/5000], Loss: 0.00264931\n",
            "Epoch [2450/5000], Loss: 0.00243465\n",
            "Epoch [2500/5000], Loss: 0.00225598\n",
            "Epoch [2550/5000], Loss: 0.00210742\n",
            "Epoch [2600/5000], Loss: 0.00198403\n",
            "Epoch [2650/5000], Loss: 0.00188165\n",
            "Epoch [2700/5000], Loss: 0.00179673\n",
            "Epoch [2750/5000], Loss: 0.00172630\n",
            "Epoch [2800/5000], Loss: 0.00166782\n",
            "Epoch [2850/5000], Loss: 0.00161919\n",
            "Epoch [2900/5000], Loss: 0.00157861\n",
            "Epoch [2950/5000], Loss: 0.00154462\n",
            "Epoch [3000/5000], Loss: 0.00151597\n",
            "Epoch [3050/5000], Loss: 0.00149165\n",
            "Epoch [3100/5000], Loss: 0.00147082\n",
            "Epoch [3150/5000], Loss: 0.00145280\n",
            "Epoch [3200/5000], Loss: 0.00143700\n",
            "Epoch [3250/5000], Loss: 0.00142299\n",
            "Epoch [3300/5000], Loss: 0.00141038\n",
            "Epoch [3350/5000], Loss: 0.00139887\n",
            "Epoch [3400/5000], Loss: 0.00138822\n",
            "Epoch [3450/5000], Loss: 0.00137824\n",
            "Epoch [3500/5000], Loss: 0.00136876\n",
            "Epoch [3550/5000], Loss: 0.00135967\n",
            "Epoch [3600/5000], Loss: 0.00135086\n",
            "Epoch [3650/5000], Loss: 0.00134226\n",
            "Epoch [3700/5000], Loss: 0.00133381\n",
            "Epoch [3750/5000], Loss: 0.00132545\n",
            "Epoch [3800/5000], Loss: 0.00131717\n",
            "Epoch [3850/5000], Loss: 0.00130891\n",
            "Epoch [3900/5000], Loss: 0.00130068\n",
            "Epoch [3950/5000], Loss: 0.00129245\n",
            "Epoch [4000/5000], Loss: 0.00128421\n",
            "Epoch [4050/5000], Loss: 0.00127595\n",
            "Epoch [4100/5000], Loss: 0.00126768\n",
            "Epoch [4150/5000], Loss: 0.00125938\n",
            "Epoch [4200/5000], Loss: 0.00125107\n",
            "Epoch [4250/5000], Loss: 0.00124273\n",
            "Epoch [4300/5000], Loss: 0.00123438\n",
            "Epoch [4350/5000], Loss: 0.00122600\n",
            "Epoch [4400/5000], Loss: 0.00121761\n",
            "Epoch [4450/5000], Loss: 0.00120921\n",
            "Epoch [4500/5000], Loss: 0.00120079\n",
            "Epoch [4550/5000], Loss: 0.00119236\n",
            "Epoch [4600/5000], Loss: 0.00118393\n",
            "Epoch [4650/5000], Loss: 0.00117549\n",
            "Epoch [4700/5000], Loss: 0.00116705\n",
            "Epoch [4750/5000], Loss: 0.00115860\n",
            "Epoch [4800/5000], Loss: 0.00115015\n",
            "Epoch [4850/5000], Loss: 0.00114170\n",
            "Epoch [4900/5000], Loss: 0.00113325\n",
            "Epoch [4950/5000], Loss: 0.00112480\n",
            "Epoch [5000/5000], Loss: 0.00111636\n",
            "Total Time Elapsed (s): 5.457895278930664\n",
            "\n",
            "Model: [50, 100]\n",
            "Epoch [1/5000], Loss: 0.80929923\n",
            "Epoch [50/5000], Loss: 0.10855693\n",
            "Epoch [100/5000], Loss: 0.10765429\n",
            "Epoch [150/5000], Loss: 0.10623858\n",
            "Epoch [200/5000], Loss: 0.10367380\n",
            "Epoch [250/5000], Loss: 0.09845596\n",
            "Epoch [300/5000], Loss: 0.08662076\n",
            "Epoch [350/5000], Loss: 0.05960444\n",
            "Epoch [400/5000], Loss: 0.02272379\n",
            "Epoch [450/5000], Loss: 0.00965402\n",
            "Epoch [500/5000], Loss: 0.00641815\n",
            "Epoch [550/5000], Loss: 0.00457655\n",
            "Epoch [600/5000], Loss: 0.00338604\n",
            "Epoch [650/5000], Loss: 0.00259253\n",
            "Epoch [700/5000], Loss: 0.00205945\n",
            "Epoch [750/5000], Loss: 0.00170108\n",
            "Epoch [800/5000], Loss: 0.00146040\n",
            "Epoch [850/5000], Loss: 0.00129885\n",
            "Epoch [900/5000], Loss: 0.00119032\n",
            "Epoch [950/5000], Loss: 0.00111718\n",
            "Epoch [1000/5000], Loss: 0.00106760\n",
            "Epoch [1050/5000], Loss: 0.00103364\n",
            "Epoch [1100/5000], Loss: 0.00101003\n",
            "Epoch [1150/5000], Loss: 0.00099327\n",
            "Epoch [1200/5000], Loss: 0.00098101\n",
            "Epoch [1250/5000], Loss: 0.00097174\n",
            "Epoch [1300/5000], Loss: 0.00096441\n",
            "Epoch [1350/5000], Loss: 0.00095837\n",
            "Epoch [1400/5000], Loss: 0.00095317\n",
            "Epoch [1450/5000], Loss: 0.00094852\n",
            "Epoch [1500/5000], Loss: 0.00094422\n",
            "Epoch [1550/5000], Loss: 0.00094015\n",
            "Epoch [1600/5000], Loss: 0.00093623\n",
            "Epoch [1650/5000], Loss: 0.00093240\n",
            "Epoch [1700/5000], Loss: 0.00092863\n",
            "Epoch [1750/5000], Loss: 0.00092490\n",
            "Epoch [1800/5000], Loss: 0.00092118\n",
            "Epoch [1850/5000], Loss: 0.00091747\n",
            "Epoch [1900/5000], Loss: 0.00091376\n",
            "Epoch [1950/5000], Loss: 0.00091006\n",
            "Epoch [2000/5000], Loss: 0.00090635\n",
            "Epoch [2050/5000], Loss: 0.00090263\n",
            "Epoch [2100/5000], Loss: 0.00089890\n",
            "Epoch [2150/5000], Loss: 0.00089517\n",
            "Epoch [2200/5000], Loss: 0.00089143\n",
            "Epoch [2250/5000], Loss: 0.00088769\n",
            "Epoch [2300/5000], Loss: 0.00088393\n",
            "Epoch [2350/5000], Loss: 0.00088017\n",
            "Epoch [2400/5000], Loss: 0.00087640\n",
            "Epoch [2450/5000], Loss: 0.00087262\n",
            "Epoch [2500/5000], Loss: 0.00086883\n",
            "Epoch [2550/5000], Loss: 0.00086503\n",
            "Epoch [2600/5000], Loss: 0.00086123\n",
            "Epoch [2650/5000], Loss: 0.00085741\n",
            "Epoch [2700/5000], Loss: 0.00085359\n",
            "Epoch [2750/5000], Loss: 0.00084977\n",
            "Epoch [2800/5000], Loss: 0.00084593\n",
            "Epoch [2850/5000], Loss: 0.00084209\n",
            "Epoch [2900/5000], Loss: 0.00083823\n",
            "Epoch [2950/5000], Loss: 0.00083437\n",
            "Epoch [3000/5000], Loss: 0.00083051\n",
            "Epoch [3050/5000], Loss: 0.00082663\n",
            "Epoch [3100/5000], Loss: 0.00082275\n",
            "Epoch [3150/5000], Loss: 0.00081886\n",
            "Epoch [3200/5000], Loss: 0.00081496\n",
            "Epoch [3250/5000], Loss: 0.00081106\n",
            "Epoch [3300/5000], Loss: 0.00080715\n",
            "Epoch [3350/5000], Loss: 0.00080323\n",
            "Epoch [3400/5000], Loss: 0.00079930\n",
            "Epoch [3450/5000], Loss: 0.00079537\n",
            "Epoch [3500/5000], Loss: 0.00079143\n",
            "Epoch [3550/5000], Loss: 0.00078749\n",
            "Epoch [3600/5000], Loss: 0.00078353\n",
            "Epoch [3650/5000], Loss: 0.00077957\n",
            "Epoch [3700/5000], Loss: 0.00077561\n",
            "Epoch [3750/5000], Loss: 0.00077164\n",
            "Epoch [3800/5000], Loss: 0.00076766\n",
            "Epoch [3850/5000], Loss: 0.00076367\n",
            "Epoch [3900/5000], Loss: 0.00075968\n",
            "Epoch [3950/5000], Loss: 0.00075569\n",
            "Epoch [4000/5000], Loss: 0.00075168\n",
            "Epoch [4050/5000], Loss: 0.00074768\n",
            "Epoch [4100/5000], Loss: 0.00074367\n",
            "Epoch [4150/5000], Loss: 0.00073965\n",
            "Epoch [4200/5000], Loss: 0.00073562\n",
            "Epoch [4250/5000], Loss: 0.00073160\n",
            "Epoch [4300/5000], Loss: 0.00072756\n",
            "Epoch [4350/5000], Loss: 0.00072352\n",
            "Epoch [4400/5000], Loss: 0.00071948\n",
            "Epoch [4450/5000], Loss: 0.00071544\n",
            "Epoch [4500/5000], Loss: 0.00071139\n",
            "Epoch [4550/5000], Loss: 0.00070733\n",
            "Epoch [4600/5000], Loss: 0.00070328\n",
            "Epoch [4650/5000], Loss: 0.00069922\n",
            "Epoch [4700/5000], Loss: 0.00069515\n",
            "Epoch [4750/5000], Loss: 0.00069108\n",
            "Epoch [4800/5000], Loss: 0.00068701\n",
            "Epoch [4850/5000], Loss: 0.00068294\n",
            "Epoch [4900/5000], Loss: 0.00067887\n",
            "Epoch [4950/5000], Loss: 0.00067479\n",
            "Epoch [5000/5000], Loss: 0.00067071\n",
            "Total Time Elapsed (s): 4.867865562438965\n",
            "\n",
            "Model: [80, 20]\n",
            "Epoch [1/5000], Loss: 0.37225315\n",
            "Epoch [50/5000], Loss: 0.10600577\n",
            "Epoch [100/5000], Loss: 0.09975398\n",
            "Epoch [150/5000], Loss: 0.08354797\n",
            "Epoch [200/5000], Loss: 0.04151233\n",
            "Epoch [250/5000], Loss: 0.01087520\n",
            "Epoch [300/5000], Loss: 0.00692577\n",
            "Epoch [350/5000], Loss: 0.00522529\n",
            "Epoch [400/5000], Loss: 0.00412688\n",
            "Epoch [450/5000], Loss: 0.00336540\n",
            "Epoch [500/5000], Loss: 0.00281939\n",
            "Epoch [550/5000], Loss: 0.00242110\n",
            "Epoch [600/5000], Loss: 0.00212771\n",
            "Epoch [650/5000], Loss: 0.00191015\n",
            "Epoch [700/5000], Loss: 0.00174786\n",
            "Epoch [750/5000], Loss: 0.00162602\n",
            "Epoch [800/5000], Loss: 0.00153381\n",
            "Epoch [850/5000], Loss: 0.00146327\n",
            "Epoch [900/5000], Loss: 0.00140856\n",
            "Epoch [950/5000], Loss: 0.00136537\n",
            "Epoch [1000/5000], Loss: 0.00133056\n",
            "Epoch [1050/5000], Loss: 0.00130180\n",
            "Epoch [1100/5000], Loss: 0.00127742\n",
            "Epoch [1150/5000], Loss: 0.00125618\n",
            "Epoch [1200/5000], Loss: 0.00123721\n",
            "Epoch [1250/5000], Loss: 0.00121988\n",
            "Epoch [1300/5000], Loss: 0.00120376\n",
            "Epoch [1350/5000], Loss: 0.00118854\n",
            "Epoch [1400/5000], Loss: 0.00117401\n",
            "Epoch [1450/5000], Loss: 0.00116004\n",
            "Epoch [1500/5000], Loss: 0.00114652\n",
            "Epoch [1550/5000], Loss: 0.00113340\n",
            "Epoch [1600/5000], Loss: 0.00112062\n",
            "Epoch [1650/5000], Loss: 0.00110817\n",
            "Epoch [1700/5000], Loss: 0.00109600\n",
            "Epoch [1750/5000], Loss: 0.00108409\n",
            "Epoch [1800/5000], Loss: 0.00107243\n",
            "Epoch [1850/5000], Loss: 0.00106101\n",
            "Epoch [1900/5000], Loss: 0.00104979\n",
            "Epoch [1950/5000], Loss: 0.00103877\n",
            "Epoch [2000/5000], Loss: 0.00102793\n",
            "Epoch [2050/5000], Loss: 0.00101726\n",
            "Epoch [2100/5000], Loss: 0.00100675\n",
            "Epoch [2150/5000], Loss: 0.00099639\n",
            "Epoch [2200/5000], Loss: 0.00098617\n",
            "Epoch [2250/5000], Loss: 0.00097608\n",
            "Epoch [2300/5000], Loss: 0.00096611\n",
            "Epoch [2350/5000], Loss: 0.00095626\n",
            "Epoch [2400/5000], Loss: 0.00094653\n",
            "Epoch [2450/5000], Loss: 0.00093690\n",
            "Epoch [2500/5000], Loss: 0.00092738\n",
            "Epoch [2550/5000], Loss: 0.00091796\n",
            "Epoch [2600/5000], Loss: 0.00090864\n",
            "Epoch [2650/5000], Loss: 0.00089941\n",
            "Epoch [2700/5000], Loss: 0.00089027\n",
            "Epoch [2750/5000], Loss: 0.00088122\n",
            "Epoch [2800/5000], Loss: 0.00087226\n",
            "Epoch [2850/5000], Loss: 0.00086338\n",
            "Epoch [2900/5000], Loss: 0.00085459\n",
            "Epoch [2950/5000], Loss: 0.00084588\n",
            "Epoch [3000/5000], Loss: 0.00083725\n",
            "Epoch [3050/5000], Loss: 0.00082870\n",
            "Epoch [3100/5000], Loss: 0.00082023\n",
            "Epoch [3150/5000], Loss: 0.00081183\n",
            "Epoch [3200/5000], Loss: 0.00080351\n",
            "Epoch [3250/5000], Loss: 0.00079527\n",
            "Epoch [3300/5000], Loss: 0.00078711\n",
            "Epoch [3350/5000], Loss: 0.00077901\n",
            "Epoch [3400/5000], Loss: 0.00077100\n",
            "Epoch [3450/5000], Loss: 0.00076305\n",
            "Epoch [3500/5000], Loss: 0.00075518\n",
            "Epoch [3550/5000], Loss: 0.00074738\n",
            "Epoch [3600/5000], Loss: 0.00073965\n",
            "Epoch [3650/5000], Loss: 0.00073200\n",
            "Epoch [3700/5000], Loss: 0.00072442\n",
            "Epoch [3750/5000], Loss: 0.00071690\n",
            "Epoch [3800/5000], Loss: 0.00070946\n",
            "Epoch [3850/5000], Loss: 0.00070209\n",
            "Epoch [3900/5000], Loss: 0.00069479\n",
            "Epoch [3950/5000], Loss: 0.00068756\n",
            "Epoch [4000/5000], Loss: 0.00068041\n",
            "Epoch [4050/5000], Loss: 0.00067332\n",
            "Epoch [4100/5000], Loss: 0.00066630\n",
            "Epoch [4150/5000], Loss: 0.00065936\n",
            "Epoch [4200/5000], Loss: 0.00065248\n",
            "Epoch [4250/5000], Loss: 0.00064568\n",
            "Epoch [4300/5000], Loss: 0.00063894\n",
            "Epoch [4350/5000], Loss: 0.00063228\n",
            "Epoch [4400/5000], Loss: 0.00062568\n",
            "Epoch [4450/5000], Loss: 0.00061916\n",
            "Epoch [4500/5000], Loss: 0.00061270\n",
            "Epoch [4550/5000], Loss: 0.00060632\n",
            "Epoch [4600/5000], Loss: 0.00060000\n",
            "Epoch [4650/5000], Loss: 0.00059376\n",
            "Epoch [4700/5000], Loss: 0.00058758\n",
            "Epoch [4750/5000], Loss: 0.00058148\n",
            "Epoch [4800/5000], Loss: 0.00057545\n",
            "Epoch [4850/5000], Loss: 0.00056948\n",
            "Epoch [4900/5000], Loss: 0.00056359\n",
            "Epoch [4950/5000], Loss: 0.00055776\n",
            "Epoch [5000/5000], Loss: 0.00055201\n",
            "Total Time Elapsed (s): 4.29575514793396\n",
            "\n",
            "Model: [100, 80]\n",
            "Epoch [1/5000], Loss: 0.21111298\n",
            "Epoch [50/5000], Loss: 0.10763869\n",
            "Epoch [100/5000], Loss: 0.10485002\n",
            "Epoch [150/5000], Loss: 0.09834027\n",
            "Epoch [200/5000], Loss: 0.07932000\n",
            "Epoch [250/5000], Loss: 0.03391503\n",
            "Epoch [300/5000], Loss: 0.00940233\n",
            "Epoch [350/5000], Loss: 0.00507464\n",
            "Epoch [400/5000], Loss: 0.00316898\n",
            "Epoch [450/5000], Loss: 0.00216550\n",
            "Epoch [500/5000], Loss: 0.00161255\n",
            "Epoch [550/5000], Loss: 0.00130186\n",
            "Epoch [600/5000], Loss: 0.00112499\n",
            "Epoch [650/5000], Loss: 0.00102290\n",
            "Epoch [700/5000], Loss: 0.00096288\n",
            "Epoch [750/5000], Loss: 0.00092659\n",
            "Epoch [800/5000], Loss: 0.00090373\n",
            "Epoch [850/5000], Loss: 0.00088847\n",
            "Epoch [900/5000], Loss: 0.00087751\n",
            "Epoch [950/5000], Loss: 0.00086897\n",
            "Epoch [1000/5000], Loss: 0.00086181\n",
            "Epoch [1050/5000], Loss: 0.00085542\n",
            "Epoch [1100/5000], Loss: 0.00084946\n",
            "Epoch [1150/5000], Loss: 0.00084374\n",
            "Epoch [1200/5000], Loss: 0.00083816\n",
            "Epoch [1250/5000], Loss: 0.00083265\n",
            "Epoch [1300/5000], Loss: 0.00082718\n",
            "Epoch [1350/5000], Loss: 0.00082173\n",
            "Epoch [1400/5000], Loss: 0.00081630\n",
            "Epoch [1450/5000], Loss: 0.00081087\n",
            "Epoch [1500/5000], Loss: 0.00080545\n",
            "Epoch [1550/5000], Loss: 0.00080003\n",
            "Epoch [1600/5000], Loss: 0.00079461\n",
            "Epoch [1650/5000], Loss: 0.00078920\n",
            "Epoch [1700/5000], Loss: 0.00078380\n",
            "Epoch [1750/5000], Loss: 0.00077839\n",
            "Epoch [1800/5000], Loss: 0.00077299\n",
            "Epoch [1850/5000], Loss: 0.00076759\n",
            "Epoch [1900/5000], Loss: 0.00076220\n",
            "Epoch [1950/5000], Loss: 0.00075681\n",
            "Epoch [2000/5000], Loss: 0.00075142\n",
            "Epoch [2050/5000], Loss: 0.00074604\n",
            "Epoch [2100/5000], Loss: 0.00074067\n",
            "Epoch [2150/5000], Loss: 0.00073530\n",
            "Epoch [2200/5000], Loss: 0.00072994\n",
            "Epoch [2250/5000], Loss: 0.00072458\n",
            "Epoch [2300/5000], Loss: 0.00071922\n",
            "Epoch [2350/5000], Loss: 0.00071387\n",
            "Epoch [2400/5000], Loss: 0.00070853\n",
            "Epoch [2450/5000], Loss: 0.00070319\n",
            "Epoch [2500/5000], Loss: 0.00069786\n",
            "Epoch [2550/5000], Loss: 0.00069253\n",
            "Epoch [2600/5000], Loss: 0.00068721\n",
            "Epoch [2650/5000], Loss: 0.00068190\n",
            "Epoch [2700/5000], Loss: 0.00067659\n",
            "Epoch [2750/5000], Loss: 0.00067129\n",
            "Epoch [2800/5000], Loss: 0.00066600\n",
            "Epoch [2850/5000], Loss: 0.00066072\n",
            "Epoch [2900/5000], Loss: 0.00065544\n",
            "Epoch [2950/5000], Loss: 0.00065017\n",
            "Epoch [3000/5000], Loss: 0.00064491\n",
            "Epoch [3050/5000], Loss: 0.00063966\n",
            "Epoch [3100/5000], Loss: 0.00063442\n",
            "Epoch [3150/5000], Loss: 0.00062918\n",
            "Epoch [3200/5000], Loss: 0.00062396\n",
            "Epoch [3250/5000], Loss: 0.00061874\n",
            "Epoch [3300/5000], Loss: 0.00061354\n",
            "Epoch [3350/5000], Loss: 0.00060835\n",
            "Epoch [3400/5000], Loss: 0.00060317\n",
            "Epoch [3450/5000], Loss: 0.00059800\n",
            "Epoch [3500/5000], Loss: 0.00059284\n",
            "Epoch [3550/5000], Loss: 0.00058770\n",
            "Epoch [3600/5000], Loss: 0.00058257\n",
            "Epoch [3650/5000], Loss: 0.00057746\n",
            "Epoch [3700/5000], Loss: 0.00057236\n",
            "Epoch [3750/5000], Loss: 0.00056727\n",
            "Epoch [3800/5000], Loss: 0.00056221\n",
            "Epoch [3850/5000], Loss: 0.00055716\n",
            "Epoch [3900/5000], Loss: 0.00055213\n",
            "Epoch [3950/5000], Loss: 0.00054711\n",
            "Epoch [4000/5000], Loss: 0.00054212\n",
            "Epoch [4050/5000], Loss: 0.00053714\n",
            "Epoch [4100/5000], Loss: 0.00053219\n",
            "Epoch [4150/5000], Loss: 0.00052726\n",
            "Epoch [4200/5000], Loss: 0.00052235\n",
            "Epoch [4250/5000], Loss: 0.00051746\n",
            "Epoch [4300/5000], Loss: 0.00051260\n",
            "Epoch [4350/5000], Loss: 0.00050776\n",
            "Epoch [4400/5000], Loss: 0.00050295\n",
            "Epoch [4450/5000], Loss: 0.00049817\n",
            "Epoch [4500/5000], Loss: 0.00049341\n",
            "Epoch [4550/5000], Loss: 0.00048868\n",
            "Epoch [4600/5000], Loss: 0.00048398\n",
            "Epoch [4650/5000], Loss: 0.00047932\n",
            "Epoch [4700/5000], Loss: 0.00047468\n",
            "Epoch [4750/5000], Loss: 0.00047008\n",
            "Epoch [4800/5000], Loss: 0.00046551\n",
            "Epoch [4850/5000], Loss: 0.00046097\n",
            "Epoch [4900/5000], Loss: 0.00045647\n",
            "Epoch [4950/5000], Loss: 0.00045200\n",
            "Epoch [5000/5000], Loss: 0.00044758\n",
            "Total Time Elapsed (s): 4.736923694610596\n",
            "\n",
            "Model: [100, 100]\n",
            "Epoch [1/5000], Loss: 0.23017994\n",
            "Epoch [50/5000], Loss: 0.10714433\n",
            "Epoch [100/5000], Loss: 0.10411204\n",
            "Epoch [150/5000], Loss: 0.09681196\n",
            "Epoch [200/5000], Loss: 0.07549313\n",
            "Epoch [250/5000], Loss: 0.02876798\n",
            "Epoch [300/5000], Loss: 0.00772562\n",
            "Epoch [350/5000], Loss: 0.00429361\n",
            "Epoch [400/5000], Loss: 0.00279419\n",
            "Epoch [450/5000], Loss: 0.00201149\n",
            "Epoch [500/5000], Loss: 0.00158244\n",
            "Epoch [550/5000], Loss: 0.00134205\n",
            "Epoch [600/5000], Loss: 0.00120531\n",
            "Epoch [650/5000], Loss: 0.00112626\n",
            "Epoch [700/5000], Loss: 0.00107953\n",
            "Epoch [750/5000], Loss: 0.00105096\n",
            "Epoch [800/5000], Loss: 0.00103260\n",
            "Epoch [850/5000], Loss: 0.00101998\n",
            "Epoch [900/5000], Loss: 0.00101058\n",
            "Epoch [950/5000], Loss: 0.00100298\n",
            "Epoch [1000/5000], Loss: 0.00099638\n",
            "Epoch [1050/5000], Loss: 0.00099032\n",
            "Epoch [1100/5000], Loss: 0.00098456\n",
            "Epoch [1150/5000], Loss: 0.00097896\n",
            "Epoch [1200/5000], Loss: 0.00097342\n",
            "Epoch [1250/5000], Loss: 0.00096791\n",
            "Epoch [1300/5000], Loss: 0.00096241\n",
            "Epoch [1350/5000], Loss: 0.00095690\n",
            "Epoch [1400/5000], Loss: 0.00095137\n",
            "Epoch [1450/5000], Loss: 0.00094583\n",
            "Epoch [1500/5000], Loss: 0.00094026\n",
            "Epoch [1550/5000], Loss: 0.00093467\n",
            "Epoch [1600/5000], Loss: 0.00092905\n",
            "Epoch [1650/5000], Loss: 0.00092342\n",
            "Epoch [1700/5000], Loss: 0.00091776\n",
            "Epoch [1750/5000], Loss: 0.00091207\n",
            "Epoch [1800/5000], Loss: 0.00090637\n",
            "Epoch [1850/5000], Loss: 0.00090065\n",
            "Epoch [1900/5000], Loss: 0.00089490\n",
            "Epoch [1950/5000], Loss: 0.00088913\n",
            "Epoch [2000/5000], Loss: 0.00088334\n",
            "Epoch [2050/5000], Loss: 0.00087753\n",
            "Epoch [2100/5000], Loss: 0.00087170\n",
            "Epoch [2150/5000], Loss: 0.00086585\n",
            "Epoch [2200/5000], Loss: 0.00085997\n",
            "Epoch [2250/5000], Loss: 0.00085408\n",
            "Epoch [2300/5000], Loss: 0.00084817\n",
            "Epoch [2350/5000], Loss: 0.00084225\n",
            "Epoch [2400/5000], Loss: 0.00083630\n",
            "Epoch [2450/5000], Loss: 0.00083034\n",
            "Epoch [2500/5000], Loss: 0.00082435\n",
            "Epoch [2550/5000], Loss: 0.00081836\n",
            "Epoch [2600/5000], Loss: 0.00081234\n",
            "Epoch [2650/5000], Loss: 0.00080631\n",
            "Epoch [2700/5000], Loss: 0.00080027\n",
            "Epoch [2750/5000], Loss: 0.00079421\n",
            "Epoch [2800/5000], Loss: 0.00078813\n",
            "Epoch [2850/5000], Loss: 0.00078205\n",
            "Epoch [2900/5000], Loss: 0.00077595\n",
            "Epoch [2950/5000], Loss: 0.00076984\n",
            "Epoch [3000/5000], Loss: 0.00076372\n",
            "Epoch [3050/5000], Loss: 0.00075759\n",
            "Epoch [3100/5000], Loss: 0.00075145\n",
            "Epoch [3150/5000], Loss: 0.00074531\n",
            "Epoch [3200/5000], Loss: 0.00073915\n",
            "Epoch [3250/5000], Loss: 0.00073299\n",
            "Epoch [3300/5000], Loss: 0.00072683\n",
            "Epoch [3350/5000], Loss: 0.00072066\n",
            "Epoch [3400/5000], Loss: 0.00071449\n",
            "Epoch [3450/5000], Loss: 0.00070832\n",
            "Epoch [3500/5000], Loss: 0.00070214\n",
            "Epoch [3550/5000], Loss: 0.00069597\n",
            "Epoch [3600/5000], Loss: 0.00068981\n",
            "Epoch [3650/5000], Loss: 0.00068364\n",
            "Epoch [3700/5000], Loss: 0.00067749\n",
            "Epoch [3750/5000], Loss: 0.00067133\n",
            "Epoch [3800/5000], Loss: 0.00066519\n",
            "Epoch [3850/5000], Loss: 0.00065906\n",
            "Epoch [3900/5000], Loss: 0.00065294\n",
            "Epoch [3950/5000], Loss: 0.00064684\n",
            "Epoch [4000/5000], Loss: 0.00064075\n",
            "Epoch [4050/5000], Loss: 0.00063467\n",
            "Epoch [4100/5000], Loss: 0.00062862\n",
            "Epoch [4150/5000], Loss: 0.00062259\n",
            "Epoch [4200/5000], Loss: 0.00061658\n",
            "Epoch [4250/5000], Loss: 0.00061059\n",
            "Epoch [4300/5000], Loss: 0.00060463\n",
            "Epoch [4350/5000], Loss: 0.00059870\n",
            "Epoch [4400/5000], Loss: 0.00059280\n",
            "Epoch [4450/5000], Loss: 0.00058694\n",
            "Epoch [4500/5000], Loss: 0.00058111\n",
            "Epoch [4550/5000], Loss: 0.00057531\n",
            "Epoch [4600/5000], Loss: 0.00056955\n",
            "Epoch [4650/5000], Loss: 0.00056384\n",
            "Epoch [4700/5000], Loss: 0.00055816\n",
            "Epoch [4750/5000], Loss: 0.00055254\n",
            "Epoch [4800/5000], Loss: 0.00054695\n",
            "Epoch [4850/5000], Loss: 0.00054142\n",
            "Epoch [4900/5000], Loss: 0.00053594\n",
            "Epoch [4950/5000], Loss: 0.00053051\n",
            "Epoch [5000/5000], Loss: 0.00052513\n",
            "Total Time Elapsed (s): 4.812922477722168\n",
            "\n",
            "Model: [100, 120]\n",
            "Epoch [1/5000], Loss: 0.22831614\n",
            "Epoch [50/5000], Loss: 0.10717131\n",
            "Epoch [100/5000], Loss: 0.10354848\n",
            "Epoch [150/5000], Loss: 0.09402908\n",
            "Epoch [200/5000], Loss: 0.06507313\n",
            "Epoch [250/5000], Loss: 0.01813616\n",
            "Epoch [300/5000], Loss: 0.00643272\n",
            "Epoch [350/5000], Loss: 0.00376724\n",
            "Epoch [400/5000], Loss: 0.00252877\n",
            "Epoch [450/5000], Loss: 0.00187473\n",
            "Epoch [500/5000], Loss: 0.00151146\n",
            "Epoch [550/5000], Loss: 0.00130380\n",
            "Epoch [600/5000], Loss: 0.00118227\n",
            "Epoch [650/5000], Loss: 0.00110928\n",
            "Epoch [700/5000], Loss: 0.00106390\n",
            "Epoch [750/5000], Loss: 0.00103433\n",
            "Epoch [800/5000], Loss: 0.00101385\n",
            "Epoch [850/5000], Loss: 0.00099859\n",
            "Epoch [900/5000], Loss: 0.00098634\n",
            "Epoch [950/5000], Loss: 0.00097584\n",
            "Epoch [1000/5000], Loss: 0.00096634\n",
            "Epoch [1050/5000], Loss: 0.00095745\n",
            "Epoch [1100/5000], Loss: 0.00094890\n",
            "Epoch [1150/5000], Loss: 0.00094058\n",
            "Epoch [1200/5000], Loss: 0.00093239\n",
            "Epoch [1250/5000], Loss: 0.00092431\n",
            "Epoch [1300/5000], Loss: 0.00091630\n",
            "Epoch [1350/5000], Loss: 0.00090834\n",
            "Epoch [1400/5000], Loss: 0.00090043\n",
            "Epoch [1450/5000], Loss: 0.00089257\n",
            "Epoch [1500/5000], Loss: 0.00088476\n",
            "Epoch [1550/5000], Loss: 0.00087698\n",
            "Epoch [1600/5000], Loss: 0.00086924\n",
            "Epoch [1650/5000], Loss: 0.00086154\n",
            "Epoch [1700/5000], Loss: 0.00085387\n",
            "Epoch [1750/5000], Loss: 0.00084625\n",
            "Epoch [1800/5000], Loss: 0.00083866\n",
            "Epoch [1850/5000], Loss: 0.00083110\n",
            "Epoch [1900/5000], Loss: 0.00082358\n",
            "Epoch [1950/5000], Loss: 0.00081609\n",
            "Epoch [2000/5000], Loss: 0.00080863\n",
            "Epoch [2050/5000], Loss: 0.00080121\n",
            "Epoch [2100/5000], Loss: 0.00079382\n",
            "Epoch [2150/5000], Loss: 0.00078646\n",
            "Epoch [2200/5000], Loss: 0.00077913\n",
            "Epoch [2250/5000], Loss: 0.00077183\n",
            "Epoch [2300/5000], Loss: 0.00076457\n",
            "Epoch [2350/5000], Loss: 0.00075733\n",
            "Epoch [2400/5000], Loss: 0.00075013\n",
            "Epoch [2450/5000], Loss: 0.00074295\n",
            "Epoch [2500/5000], Loss: 0.00073581\n",
            "Epoch [2550/5000], Loss: 0.00072869\n",
            "Epoch [2600/5000], Loss: 0.00072161\n",
            "Epoch [2650/5000], Loss: 0.00071455\n",
            "Epoch [2700/5000], Loss: 0.00070753\n",
            "Epoch [2750/5000], Loss: 0.00070053\n",
            "Epoch [2800/5000], Loss: 0.00069356\n",
            "Epoch [2850/5000], Loss: 0.00068663\n",
            "Epoch [2900/5000], Loss: 0.00067972\n",
            "Epoch [2950/5000], Loss: 0.00067285\n",
            "Epoch [3000/5000], Loss: 0.00066601\n",
            "Epoch [3050/5000], Loss: 0.00065920\n",
            "Epoch [3100/5000], Loss: 0.00065242\n",
            "Epoch [3150/5000], Loss: 0.00064568\n",
            "Epoch [3200/5000], Loss: 0.00063897\n",
            "Epoch [3250/5000], Loss: 0.00063229\n",
            "Epoch [3300/5000], Loss: 0.00062565\n",
            "Epoch [3350/5000], Loss: 0.00061905\n",
            "Epoch [3400/5000], Loss: 0.00061248\n",
            "Epoch [3450/5000], Loss: 0.00060595\n",
            "Epoch [3500/5000], Loss: 0.00059946\n",
            "Epoch [3550/5000], Loss: 0.00059300\n",
            "Epoch [3600/5000], Loss: 0.00058659\n",
            "Epoch [3650/5000], Loss: 0.00058023\n",
            "Epoch [3700/5000], Loss: 0.00057390\n",
            "Epoch [3750/5000], Loss: 0.00056762\n",
            "Epoch [3800/5000], Loss: 0.00056138\n",
            "Epoch [3850/5000], Loss: 0.00055519\n",
            "Epoch [3900/5000], Loss: 0.00054905\n",
            "Epoch [3950/5000], Loss: 0.00054295\n",
            "Epoch [4000/5000], Loss: 0.00053691\n",
            "Epoch [4050/5000], Loss: 0.00053092\n",
            "Epoch [4100/5000], Loss: 0.00052498\n",
            "Epoch [4150/5000], Loss: 0.00051909\n",
            "Epoch [4200/5000], Loss: 0.00051326\n",
            "Epoch [4250/5000], Loss: 0.00050749\n",
            "Epoch [4300/5000], Loss: 0.00050178\n",
            "Epoch [4350/5000], Loss: 0.00049612\n",
            "Epoch [4400/5000], Loss: 0.00049053\n",
            "Epoch [4450/5000], Loss: 0.00048500\n",
            "Epoch [4500/5000], Loss: 0.00047953\n",
            "Epoch [4550/5000], Loss: 0.00047413\n",
            "Epoch [4600/5000], Loss: 0.00046879\n",
            "Epoch [4650/5000], Loss: 0.00046352\n",
            "Epoch [4700/5000], Loss: 0.00045832\n",
            "Epoch [4750/5000], Loss: 0.00045318\n",
            "Epoch [4800/5000], Loss: 0.00044812\n",
            "Epoch [4850/5000], Loss: 0.00044313\n",
            "Epoch [4900/5000], Loss: 0.00043821\n",
            "Epoch [4950/5000], Loss: 0.00043336\n",
            "Epoch [5000/5000], Loss: 0.00042859\n",
            "Total Time Elapsed (s): 5.106760263442993\n",
            "\n",
            "Model: [100, 150]\n",
            "Epoch [1/5000], Loss: 0.52620155\n",
            "Epoch [50/5000], Loss: 0.10907277\n",
            "Epoch [100/5000], Loss: 0.10901745\n",
            "Epoch [150/5000], Loss: 0.10897497\n",
            "Epoch [200/5000], Loss: 0.10893525\n",
            "Epoch [250/5000], Loss: 0.10889068\n",
            "Epoch [300/5000], Loss: 0.10883275\n",
            "Epoch [350/5000], Loss: 0.10874801\n",
            "Epoch [400/5000], Loss: 0.10861004\n",
            "Epoch [450/5000], Loss: 0.10835803\n",
            "Epoch [500/5000], Loss: 0.10782155\n",
            "Epoch [550/5000], Loss: 0.10636045\n",
            "Epoch [600/5000], Loss: 0.10056719\n",
            "Epoch [650/5000], Loss: 0.07990784\n",
            "Epoch [700/5000], Loss: 0.02977942\n",
            "Epoch [750/5000], Loss: 0.01028732\n",
            "Epoch [800/5000], Loss: 0.00778946\n",
            "Epoch [850/5000], Loss: 0.00607155\n",
            "Epoch [900/5000], Loss: 0.00473792\n",
            "Epoch [950/5000], Loss: 0.00372188\n",
            "Epoch [1000/5000], Loss: 0.00296751\n",
            "Epoch [1050/5000], Loss: 0.00241901\n",
            "Epoch [1100/5000], Loss: 0.00202692\n",
            "Epoch [1150/5000], Loss: 0.00175015\n",
            "Epoch [1200/5000], Loss: 0.00155619\n",
            "Epoch [1250/5000], Loss: 0.00142038\n",
            "Epoch [1300/5000], Loss: 0.00132463\n",
            "Epoch [1350/5000], Loss: 0.00125606\n",
            "Epoch [1400/5000], Loss: 0.00120567\n",
            "Epoch [1450/5000], Loss: 0.00116732\n",
            "Epoch [1500/5000], Loss: 0.00113689\n",
            "Epoch [1550/5000], Loss: 0.00111167\n",
            "Epoch [1600/5000], Loss: 0.00108991\n",
            "Epoch [1650/5000], Loss: 0.00107049\n",
            "Epoch [1700/5000], Loss: 0.00105273\n",
            "Epoch [1750/5000], Loss: 0.00103620\n",
            "Epoch [1800/5000], Loss: 0.00102062\n",
            "Epoch [1850/5000], Loss: 0.00100582\n",
            "Epoch [1900/5000], Loss: 0.00099169\n",
            "Epoch [1950/5000], Loss: 0.00097815\n",
            "Epoch [2000/5000], Loss: 0.00096514\n",
            "Epoch [2050/5000], Loss: 0.00095261\n",
            "Epoch [2100/5000], Loss: 0.00094052\n",
            "Epoch [2150/5000], Loss: 0.00092884\n",
            "Epoch [2200/5000], Loss: 0.00091752\n",
            "Epoch [2250/5000], Loss: 0.00090655\n",
            "Epoch [2300/5000], Loss: 0.00089588\n",
            "Epoch [2350/5000], Loss: 0.00088551\n",
            "Epoch [2400/5000], Loss: 0.00087540\n",
            "Epoch [2450/5000], Loss: 0.00086553\n",
            "Epoch [2500/5000], Loss: 0.00085589\n",
            "Epoch [2550/5000], Loss: 0.00084645\n",
            "Epoch [2600/5000], Loss: 0.00083719\n",
            "Epoch [2650/5000], Loss: 0.00082811\n",
            "Epoch [2700/5000], Loss: 0.00081919\n",
            "Epoch [2750/5000], Loss: 0.00081040\n",
            "Epoch [2800/5000], Loss: 0.00080175\n",
            "Epoch [2850/5000], Loss: 0.00079323\n",
            "Epoch [2900/5000], Loss: 0.00078481\n",
            "Epoch [2950/5000], Loss: 0.00077650\n",
            "Epoch [3000/5000], Loss: 0.00076829\n",
            "Epoch [3050/5000], Loss: 0.00076017\n",
            "Epoch [3100/5000], Loss: 0.00075212\n",
            "Epoch [3150/5000], Loss: 0.00074416\n",
            "Epoch [3200/5000], Loss: 0.00073627\n",
            "Epoch [3250/5000], Loss: 0.00072845\n",
            "Epoch [3300/5000], Loss: 0.00072070\n",
            "Epoch [3350/5000], Loss: 0.00071301\n",
            "Epoch [3400/5000], Loss: 0.00070538\n",
            "Epoch [3450/5000], Loss: 0.00069780\n",
            "Epoch [3500/5000], Loss: 0.00069029\n",
            "Epoch [3550/5000], Loss: 0.00068283\n",
            "Epoch [3600/5000], Loss: 0.00067543\n",
            "Epoch [3650/5000], Loss: 0.00066808\n",
            "Epoch [3700/5000], Loss: 0.00066078\n",
            "Epoch [3750/5000], Loss: 0.00065354\n",
            "Epoch [3800/5000], Loss: 0.00064636\n",
            "Epoch [3850/5000], Loss: 0.00063923\n",
            "Epoch [3900/5000], Loss: 0.00063215\n",
            "Epoch [3950/5000], Loss: 0.00062514\n",
            "Epoch [4000/5000], Loss: 0.00061818\n",
            "Epoch [4050/5000], Loss: 0.00061128\n",
            "Epoch [4100/5000], Loss: 0.00060444\n",
            "Epoch [4150/5000], Loss: 0.00059766\n",
            "Epoch [4200/5000], Loss: 0.00059094\n",
            "Epoch [4250/5000], Loss: 0.00058429\n",
            "Epoch [4300/5000], Loss: 0.00057771\n",
            "Epoch [4350/5000], Loss: 0.00057120\n",
            "Epoch [4400/5000], Loss: 0.00056476\n",
            "Epoch [4450/5000], Loss: 0.00055839\n",
            "Epoch [4500/5000], Loss: 0.00055209\n",
            "Epoch [4550/5000], Loss: 0.00054587\n",
            "Epoch [4600/5000], Loss: 0.00053973\n",
            "Epoch [4650/5000], Loss: 0.00053367\n",
            "Epoch [4700/5000], Loss: 0.00052769\n",
            "Epoch [4750/5000], Loss: 0.00052179\n",
            "Epoch [4800/5000], Loss: 0.00051598\n",
            "Epoch [4850/5000], Loss: 0.00051025\n",
            "Epoch [4900/5000], Loss: 0.00050461\n",
            "Epoch [4950/5000], Loss: 0.00049906\n",
            "Epoch [5000/5000], Loss: 0.00049361\n",
            "Total Time Elapsed (s): 5.278343200683594\n",
            "\n",
            "Model: [500, 100]\n",
            "Epoch [1/5000], Loss: 0.33187458\n",
            "Epoch [50/5000], Loss: 0.09644475\n",
            "Epoch [100/5000], Loss: 0.03475765\n",
            "Epoch [150/5000], Loss: 0.00591425\n",
            "Epoch [200/5000], Loss: 0.00324807\n",
            "Epoch [250/5000], Loss: 0.00220118\n",
            "Epoch [300/5000], Loss: 0.00169791\n",
            "Epoch [350/5000], Loss: 0.00143689\n",
            "Epoch [400/5000], Loss: 0.00129257\n",
            "Epoch [450/5000], Loss: 0.00120545\n",
            "Epoch [500/5000], Loss: 0.00114630\n",
            "Epoch [550/5000], Loss: 0.00110077\n",
            "Epoch [600/5000], Loss: 0.00106201\n",
            "Epoch [650/5000], Loss: 0.00102684\n",
            "Epoch [700/5000], Loss: 0.00099391\n",
            "Epoch [750/5000], Loss: 0.00096267\n",
            "Epoch [800/5000], Loss: 0.00093292\n",
            "Epoch [850/5000], Loss: 0.00090455\n",
            "Epoch [900/5000], Loss: 0.00087751\n",
            "Epoch [950/5000], Loss: 0.00085173\n",
            "Epoch [1000/5000], Loss: 0.00082716\n",
            "Epoch [1050/5000], Loss: 0.00080371\n",
            "Epoch [1100/5000], Loss: 0.00078134\n",
            "Epoch [1150/5000], Loss: 0.00075998\n",
            "Epoch [1200/5000], Loss: 0.00073958\n",
            "Epoch [1250/5000], Loss: 0.00072008\n",
            "Epoch [1300/5000], Loss: 0.00070143\n",
            "Epoch [1350/5000], Loss: 0.00068359\n",
            "Epoch [1400/5000], Loss: 0.00066652\n",
            "Epoch [1450/5000], Loss: 0.00065017\n",
            "Epoch [1500/5000], Loss: 0.00063452\n",
            "Epoch [1550/5000], Loss: 0.00061951\n",
            "Epoch [1600/5000], Loss: 0.00060513\n",
            "Epoch [1650/5000], Loss: 0.00059134\n",
            "Epoch [1700/5000], Loss: 0.00057812\n",
            "Epoch [1750/5000], Loss: 0.00056544\n",
            "Epoch [1800/5000], Loss: 0.00055328\n",
            "Epoch [1850/5000], Loss: 0.00054161\n",
            "Epoch [1900/5000], Loss: 0.00053042\n",
            "Epoch [1950/5000], Loss: 0.00051968\n",
            "Epoch [2000/5000], Loss: 0.00050938\n",
            "Epoch [2050/5000], Loss: 0.00049951\n",
            "Epoch [2100/5000], Loss: 0.00049004\n",
            "Epoch [2150/5000], Loss: 0.00048097\n",
            "Epoch [2200/5000], Loss: 0.00047227\n",
            "Epoch [2250/5000], Loss: 0.00046394\n",
            "Epoch [2300/5000], Loss: 0.00045597\n",
            "Epoch [2350/5000], Loss: 0.00044833\n",
            "Epoch [2400/5000], Loss: 0.00044102\n",
            "Epoch [2450/5000], Loss: 0.00043402\n",
            "Epoch [2500/5000], Loss: 0.00042733\n",
            "Epoch [2550/5000], Loss: 0.00042094\n",
            "Epoch [2600/5000], Loss: 0.00041483\n",
            "Epoch [2650/5000], Loss: 0.00040899\n",
            "Epoch [2700/5000], Loss: 0.00040342\n",
            "Epoch [2750/5000], Loss: 0.00039810\n",
            "Epoch [2800/5000], Loss: 0.00039303\n",
            "Epoch [2850/5000], Loss: 0.00038818\n",
            "Epoch [2900/5000], Loss: 0.00038357\n",
            "Epoch [2950/5000], Loss: 0.00037916\n",
            "Epoch [3000/5000], Loss: 0.00037497\n",
            "Epoch [3050/5000], Loss: 0.00037097\n",
            "Epoch [3100/5000], Loss: 0.00036716\n",
            "Epoch [3150/5000], Loss: 0.00036353\n",
            "Epoch [3200/5000], Loss: 0.00036007\n",
            "Epoch [3250/5000], Loss: 0.00035678\n",
            "Epoch [3300/5000], Loss: 0.00035365\n",
            "Epoch [3350/5000], Loss: 0.00035066\n",
            "Epoch [3400/5000], Loss: 0.00034782\n",
            "Epoch [3450/5000], Loss: 0.00034511\n",
            "Epoch [3500/5000], Loss: 0.00034253\n",
            "Epoch [3550/5000], Loss: 0.00034007\n",
            "Epoch [3600/5000], Loss: 0.00033773\n",
            "Epoch [3650/5000], Loss: 0.00033549\n",
            "Epoch [3700/5000], Loss: 0.00033336\n",
            "Epoch [3750/5000], Loss: 0.00033133\n",
            "Epoch [3800/5000], Loss: 0.00032939\n",
            "Epoch [3850/5000], Loss: 0.00032754\n",
            "Epoch [3900/5000], Loss: 0.00032577\n",
            "Epoch [3950/5000], Loss: 0.00032408\n",
            "Epoch [4000/5000], Loss: 0.00032247\n",
            "Epoch [4050/5000], Loss: 0.00032092\n",
            "Epoch [4100/5000], Loss: 0.00031944\n",
            "Epoch [4150/5000], Loss: 0.00031802\n",
            "Epoch [4200/5000], Loss: 0.00031666\n",
            "Epoch [4250/5000], Loss: 0.00031536\n",
            "Epoch [4300/5000], Loss: 0.00031410\n",
            "Epoch [4350/5000], Loss: 0.00031290\n",
            "Epoch [4400/5000], Loss: 0.00031174\n",
            "Epoch [4450/5000], Loss: 0.00031062\n",
            "Epoch [4500/5000], Loss: 0.00030955\n",
            "Epoch [4550/5000], Loss: 0.00030851\n",
            "Epoch [4600/5000], Loss: 0.00030751\n",
            "Epoch [4650/5000], Loss: 0.00030654\n",
            "Epoch [4700/5000], Loss: 0.00030561\n",
            "Epoch [4750/5000], Loss: 0.00030470\n",
            "Epoch [4800/5000], Loss: 0.00030383\n",
            "Epoch [4850/5000], Loss: 0.00030298\n",
            "Epoch [4900/5000], Loss: 0.00030215\n",
            "Epoch [4950/5000], Loss: 0.00030135\n",
            "Epoch [5000/5000], Loss: 0.00030057\n",
            "Total Time Elapsed (s): 7.834435224533081\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3jic4oy4DcJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}