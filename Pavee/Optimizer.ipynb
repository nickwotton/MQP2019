{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Optimizer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pphongsopa/MQP2019/blob/master/Pavee/Optimizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkst88Odo0h6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#neccessary imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZaWK-ORK6tH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#function that we want to approximate\n",
        "f = lambda x: x**2+1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4GrHOgjfK74D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#initializing variables for\n",
        "#Trial Range\n",
        "maxNeuron = 200\n",
        "maxLayer = 3\n",
        "\n",
        "#Best set up for least loss\n",
        "bestLoss = 100\n",
        "bestNeurons = 0\n",
        "bestLayers = 0\n",
        "bestTimeForLoss = 1000000 #i dont know what Max_int equivalent is\n",
        "\n",
        "#Best set up for least time within < 0.001\n",
        "neuronsBestTime = 0\n",
        "layersBestTime = 0\n",
        "lossBestTime = 1000000 #same as the above\n",
        "\n",
        "#temporary variables\n",
        "tempTime = 0\n",
        "tempLoss = 0\n",
        "\n",
        "#other variables\n",
        "learning_rate = 0.001\n",
        "num_epochs = 10000\n",
        "batch_size = 100\n",
        "lossThreshold = 0.01"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGcI6dDHfDk1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#loss function \n",
        "criterion = nn.MSELoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxHIsBu2fLUa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#training data\n",
        "batch_size  = 100\n",
        "x_train = torch.randn(batch_size, 1)\n",
        "y_train = f(x_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_vr7DY3fIAV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#optimizer\n",
        "learning_rate = 0.001\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Pep9K65S-6j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "481f731f-d042-4224-eaff-1a3756f6fa92"
      },
      "source": [
        "#Best set up for least loss\n",
        "bestLoss = 100\n",
        "bestNeurons = 0\n",
        "bestLayers = 0\n",
        "bestTimeForLoss = 1000000 #i dont know what Max_int equivalent is\n",
        "\n",
        "#Best set up for least time within < 0.001\n",
        "neuronsBestTime = -1\n",
        "layersBestTime = -1\n",
        "lossBestTime = 1000000 #same as the above\n",
        "\n",
        "#temporary variables\n",
        "tempTime = 0\n",
        "tempLoss = 0\n",
        "\n",
        "#main code/model\n",
        "for i in range(2, maxNeuron+1): #loop through each # of neurons\n",
        "  for j in range(2,maxLayer+1): #loop through each # of layers\n",
        "    model = nn.Sequential(\n",
        "      nn.Linear(1, i),\n",
        "      nn.Sigmoid(),\n",
        "      nn.Linear(i, i),\n",
        "      #loop for the layers inside \n",
        "      #need help fixing this**\n",
        "      #for k in range(2, j):\n",
        "       # nn.Linear(i, i),\n",
        "      nn.Linear(i, 1)\n",
        "    )\n",
        "    \n",
        "    #run the rest of the code with the current neurons and layers set up\n",
        "    \n",
        "    #start the timer \n",
        "    start = time.time()\n",
        "    \n",
        "    # Train the model\n",
        "    for epoch in range(10000):\n",
        "\n",
        "      # Forward pass\n",
        "      outputs = model(x_train)\n",
        "      loss = criterion(outputs, y_train)\n",
        "      \n",
        "      # Backward and optimize\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      \n",
        "      \n",
        "      if (epoch+1) % 10000 == 0:\n",
        "        print ('Loss: ', loss.item())\n",
        "    #end trainging loop\n",
        "    \n",
        "    #stop the timer\n",
        "    end = time.time()\n",
        "    \n",
        "    #temp variables\n",
        "    tempLoss = loss.item()\n",
        "    tempTime = end - start\n",
        "          \n",
        "    #Get time and other statistics if the requirements are met \n",
        "    #if statement for least loss\n",
        "    if bestLoss > tempLoss :\n",
        "      bestLoss = tempLoss\n",
        "      bestNeurons = i\n",
        "      bestLayers = j\n",
        "      bestTimeForLoss = tempTime\n",
        "        \n",
        "    #if statement for best time when loss is < 0.001 \n",
        "    if tempLoss < lossThreshold and lossBestTime > tempTime:\n",
        "      neuronsBestTime = i\n",
        "      layersBestTime = j\n",
        "      lossBestTime = tempTime\n",
        "      LossLoss = tempLoss\n",
        "#end of Test Runs\n",
        "\n",
        "#print out saved statistic\n",
        "print ('------------------------------------------------')\n",
        "print ('Setting for least loss')\n",
        "print ('Least Loss: ', bestLoss)\n",
        "print ('Neurons: ', bestNeurons)\n",
        "print ('Layers: ', bestLayers)\n",
        "print ('Time: ', bestTimeForLoss,'s')\n",
        "print (' ')\n",
        "print ('Setting for least time under 0.001 loss')\n",
        "print ('Loss: ', LossLoss)\n",
        "print ('Neurons: ', neuronsBestTime)\n",
        "print ('Layers: ', layersBestTime)\n",
        "print ('Time: ', lossBestTime, 's')\n",
        "\n",
        "          \n",
        "  \n",
        "        "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss:  8.273691177368164\n",
            "Loss:  4.345492839813232\n",
            "Loss:  5.781960964202881\n",
            "Loss:  5.161777019500732\n",
            "Loss:  7.496591567993164\n",
            "Loss:  6.694154739379883\n",
            "Loss:  4.1640520095825195\n",
            "Loss:  6.852205276489258\n",
            "Loss:  6.021041393280029\n",
            "Loss:  6.275326251983643\n",
            "Loss:  7.8746137619018555\n",
            "Loss:  6.534112930297852\n",
            "Loss:  9.1558256149292\n",
            "Loss:  5.897417068481445\n",
            "Loss:  8.748132705688477\n",
            "Loss:  5.604424953460693\n",
            "Loss:  6.394999980926514\n",
            "Loss:  9.07784366607666\n",
            "Loss:  6.543118000030518\n",
            "Loss:  7.064272403717041\n",
            "Loss:  6.855663299560547\n",
            "Loss:  6.810102462768555\n",
            "Loss:  7.243194103240967\n",
            "Loss:  5.973698616027832\n",
            "Loss:  6.232954025268555\n",
            "Loss:  5.624210357666016\n",
            "Loss:  6.730460166931152\n",
            "Loss:  8.20871353149414\n",
            "Loss:  6.780972480773926\n",
            "Loss:  7.645307540893555\n",
            "Loss:  7.059112548828125\n",
            "Loss:  6.33618688583374\n",
            "Loss:  7.4991607666015625\n",
            "Loss:  7.443195343017578\n",
            "Loss:  6.066929340362549\n",
            "Loss:  6.112716197967529\n",
            "Loss:  6.62040901184082\n",
            "Loss:  7.480795383453369\n",
            "Loss:  8.3231782913208\n",
            "Loss:  7.640299320220947\n",
            "Loss:  7.829676628112793\n",
            "Loss:  6.7452473640441895\n",
            "Loss:  7.636384963989258\n",
            "Loss:  6.883474349975586\n",
            "Loss:  6.005813121795654\n",
            "Loss:  6.142076015472412\n",
            "Loss:  6.417472839355469\n",
            "Loss:  6.861897945404053\n",
            "Loss:  6.17142391204834\n",
            "Loss:  4.874629497528076\n",
            "Loss:  9.571578979492188\n",
            "Loss:  8.493005752563477\n",
            "Loss:  7.187002182006836\n",
            "Loss:  6.962462902069092\n",
            "Loss:  5.89504861831665\n",
            "Loss:  6.516242980957031\n",
            "Loss:  6.81003999710083\n",
            "Loss:  4.916853904724121\n",
            "Loss:  5.610494613647461\n",
            "Loss:  5.49222469329834\n",
            "Loss:  6.923213958740234\n",
            "Loss:  6.721739292144775\n",
            "Loss:  8.042635917663574\n",
            "Loss:  7.259785175323486\n",
            "Loss:  7.690347194671631\n",
            "Loss:  8.646430015563965\n",
            "Loss:  7.458705425262451\n",
            "Loss:  6.647346496582031\n",
            "Loss:  6.950953483581543\n",
            "Loss:  6.5938401222229\n",
            "Loss:  7.1030683517456055\n",
            "Loss:  5.649475574493408\n",
            "Loss:  7.299623489379883\n",
            "Loss:  6.268176555633545\n",
            "Loss:  7.0767316818237305\n",
            "Loss:  6.449601650238037\n",
            "Loss:  6.492027759552002\n",
            "Loss:  7.45239782333374\n",
            "Loss:  8.320779800415039\n",
            "Loss:  7.187918663024902\n",
            "Loss:  6.08353853225708\n",
            "Loss:  7.748762130737305\n",
            "Loss:  5.915184497833252\n",
            "Loss:  5.325599670410156\n",
            "Loss:  6.314319610595703\n",
            "Loss:  5.9588398933410645\n",
            "Loss:  5.387781143188477\n",
            "Loss:  6.354598522186279\n",
            "Loss:  5.502071380615234\n",
            "Loss:  7.181607723236084\n",
            "Loss:  7.604465484619141\n",
            "Loss:  5.790462493896484\n",
            "Loss:  6.177145957946777\n",
            "Loss:  6.963923931121826\n",
            "Loss:  7.252302646636963\n",
            "Loss:  6.229936122894287\n",
            "Loss:  6.611240386962891\n",
            "Loss:  6.036003112792969\n",
            "Loss:  7.398412704467773\n",
            "Loss:  6.163957118988037\n",
            "Loss:  5.717369079589844\n",
            "Loss:  7.773769378662109\n",
            "Loss:  5.514288425445557\n",
            "Loss:  9.13149356842041\n",
            "Loss:  7.065286159515381\n",
            "Loss:  6.210019111633301\n",
            "Loss:  7.085214614868164\n",
            "Loss:  8.194759368896484\n",
            "Loss:  6.644800662994385\n",
            "Loss:  6.510979175567627\n",
            "Loss:  6.658414363861084\n",
            "Loss:  6.2500433921813965\n",
            "Loss:  6.793398380279541\n",
            "Loss:  6.416248798370361\n",
            "Loss:  6.867020606994629\n",
            "Loss:  8.288749694824219\n",
            "Loss:  5.5123796463012695\n",
            "Loss:  7.0871453285217285\n",
            "Loss:  7.4204511642456055\n",
            "Loss:  5.834848403930664\n",
            "Loss:  6.902309417724609\n",
            "Loss:  7.397461891174316\n",
            "Loss:  6.991433620452881\n",
            "Loss:  7.2509331703186035\n",
            "Loss:  8.058219909667969\n",
            "Loss:  6.943830490112305\n",
            "Loss:  5.8160600662231445\n",
            "Loss:  7.962123394012451\n",
            "Loss:  6.5750861167907715\n",
            "Loss:  5.9850287437438965\n",
            "Loss:  7.271961688995361\n",
            "Loss:  6.143492698669434\n",
            "Loss:  7.464004039764404\n",
            "Loss:  6.591338157653809\n",
            "Loss:  6.390665531158447\n",
            "Loss:  6.025787830352783\n",
            "Loss:  6.003034591674805\n",
            "Loss:  5.142478942871094\n",
            "Loss:  6.630301475524902\n",
            "Loss:  7.712413311004639\n",
            "Loss:  7.120681285858154\n",
            "Loss:  5.276603698730469\n",
            "Loss:  5.916696071624756\n",
            "Loss:  6.3480224609375\n",
            "Loss:  7.130383491516113\n",
            "Loss:  6.150561809539795\n",
            "Loss:  6.985696792602539\n",
            "Loss:  6.487363815307617\n",
            "Loss:  6.154972553253174\n",
            "Loss:  6.442257881164551\n",
            "Loss:  6.610811233520508\n",
            "Loss:  5.300175666809082\n",
            "Loss:  6.500627040863037\n",
            "Loss:  7.802497386932373\n",
            "Loss:  6.044760227203369\n",
            "Loss:  6.109330654144287\n",
            "Loss:  5.730649948120117\n",
            "Loss:  6.252908229827881\n",
            "Loss:  6.023720741271973\n",
            "Loss:  6.117294788360596\n",
            "Loss:  6.4744648933410645\n",
            "Loss:  6.5039520263671875\n",
            "Loss:  5.934111595153809\n",
            "Loss:  6.651837348937988\n",
            "Loss:  6.223999500274658\n",
            "Loss:  6.215946197509766\n",
            "Loss:  7.491415500640869\n",
            "Loss:  6.781872272491455\n",
            "Loss:  7.700667381286621\n",
            "Loss:  6.204606533050537\n",
            "Loss:  6.634668827056885\n",
            "Loss:  6.751693248748779\n",
            "Loss:  7.6365275382995605\n",
            "Loss:  6.107673168182373\n",
            "Loss:  5.690096855163574\n",
            "Loss:  7.026617527008057\n",
            "Loss:  7.638372898101807\n",
            "Loss:  7.646335124969482\n",
            "Loss:  6.42112398147583\n",
            "Loss:  5.689133644104004\n",
            "Loss:  6.664344310760498\n",
            "Loss:  6.8009257316589355\n",
            "Loss:  5.1343302726745605\n",
            "Loss:  7.123205661773682\n",
            "Loss:  8.309374809265137\n",
            "Loss:  8.010461807250977\n",
            "Loss:  6.87674617767334\n",
            "Loss:  6.2336835861206055\n",
            "Loss:  5.7060699462890625\n",
            "Loss:  7.32971715927124\n",
            "Loss:  8.36300277709961\n",
            "Loss:  7.277543544769287\n",
            "Loss:  5.205859661102295\n",
            "Loss:  7.380741596221924\n",
            "Loss:  7.047479629516602\n",
            "Loss:  6.8457159996032715\n",
            "Loss:  5.2404069900512695\n",
            "Loss:  6.68054723739624\n",
            "Loss:  7.897580146789551\n",
            "Loss:  7.061892986297607\n",
            "Loss:  7.813113212585449\n",
            "Loss:  7.527027130126953\n",
            "Loss:  6.569308757781982\n",
            "Loss:  6.8918914794921875\n",
            "Loss:  7.746258735656738\n",
            "Loss:  5.987118721008301\n",
            "Loss:  6.485221862792969\n",
            "Loss:  6.817292213439941\n",
            "Loss:  5.110556602478027\n",
            "Loss:  7.521390914916992\n",
            "Loss:  6.457457065582275\n",
            "Loss:  6.1226606369018555\n",
            "Loss:  8.130743980407715\n",
            "Loss:  8.495244979858398\n",
            "Loss:  5.994014739990234\n",
            "Loss:  6.949731349945068\n",
            "Loss:  6.163947582244873\n",
            "Loss:  6.010974407196045\n",
            "Loss:  7.512617588043213\n",
            "Loss:  6.598033428192139\n",
            "Loss:  6.296117782592773\n",
            "Loss:  7.118441581726074\n",
            "Loss:  6.212522506713867\n",
            "Loss:  5.786967754364014\n",
            "Loss:  4.656203746795654\n",
            "Loss:  7.346864700317383\n",
            "Loss:  6.507157802581787\n",
            "Loss:  7.249608516693115\n",
            "Loss:  7.253888130187988\n",
            "Loss:  5.955817222595215\n",
            "Loss:  5.760315895080566\n",
            "Loss:  6.68232536315918\n",
            "Loss:  6.697447776794434\n",
            "Loss:  7.509453773498535\n",
            "Loss:  6.957417964935303\n",
            "Loss:  6.526932716369629\n",
            "Loss:  6.123517990112305\n",
            "Loss:  8.250893592834473\n",
            "Loss:  7.530170917510986\n",
            "Loss:  5.693867206573486\n",
            "Loss:  6.543876647949219\n",
            "Loss:  7.151073455810547\n",
            "Loss:  6.653448104858398\n",
            "Loss:  6.47871208190918\n",
            "Loss:  5.749446392059326\n",
            "Loss:  6.071563720703125\n",
            "Loss:  7.032578945159912\n",
            "Loss:  5.4658637046813965\n",
            "Loss:  7.146705150604248\n",
            "Loss:  8.040040969848633\n",
            "Loss:  6.492813587188721\n",
            "Loss:  7.062950611114502\n",
            "Loss:  6.846424579620361\n",
            "Loss:  6.9196038246154785\n",
            "Loss:  5.8033366203308105\n",
            "Loss:  8.61705207824707\n",
            "Loss:  8.115011215209961\n",
            "Loss:  7.773762226104736\n",
            "Loss:  6.960718154907227\n",
            "Loss:  7.1055827140808105\n",
            "Loss:  7.821224689483643\n",
            "Loss:  7.914928913116455\n",
            "Loss:  6.477373123168945\n",
            "Loss:  6.425137042999268\n",
            "Loss:  7.82871675491333\n",
            "Loss:  6.689881801605225\n",
            "Loss:  6.487091541290283\n",
            "Loss:  5.338291645050049\n",
            "Loss:  8.100303649902344\n",
            "Loss:  5.306641578674316\n",
            "Loss:  7.242804050445557\n",
            "Loss:  7.362776756286621\n",
            "Loss:  7.047554969787598\n",
            "Loss:  6.151422500610352\n",
            "Loss:  6.38877534866333\n",
            "Loss:  5.576799392700195\n",
            "Loss:  8.455513000488281\n",
            "Loss:  6.638809680938721\n",
            "Loss:  8.07291316986084\n",
            "Loss:  7.212946891784668\n",
            "Loss:  7.367961883544922\n",
            "Loss:  6.936717987060547\n",
            "Loss:  6.383888244628906\n",
            "Loss:  7.832557678222656\n",
            "Loss:  6.471113681793213\n",
            "Loss:  7.675379276275635\n",
            "Loss:  6.179667949676514\n",
            "Loss:  8.184576034545898\n",
            "Loss:  7.27988338470459\n",
            "Loss:  6.959900379180908\n",
            "Loss:  7.04587459564209\n",
            "Loss:  5.570058822631836\n",
            "Loss:  7.076861381530762\n",
            "Loss:  6.809940338134766\n",
            "Loss:  5.981071949005127\n",
            "Loss:  6.824283123016357\n",
            "Loss:  6.691575527191162\n",
            "Loss:  7.498181343078613\n",
            "Loss:  7.2398810386657715\n",
            "Loss:  6.953073501586914\n",
            "Loss:  6.606908321380615\n",
            "Loss:  5.184226989746094\n",
            "Loss:  7.1275763511657715\n",
            "Loss:  5.732937812805176\n",
            "Loss:  8.246198654174805\n",
            "Loss:  5.884985446929932\n",
            "Loss:  6.090848922729492\n",
            "Loss:  7.590822219848633\n",
            "Loss:  7.380273342132568\n",
            "Loss:  6.015132904052734\n",
            "Loss:  6.970180034637451\n",
            "Loss:  6.726047992706299\n",
            "Loss:  6.60413122177124\n",
            "Loss:  7.178738594055176\n",
            "Loss:  7.126069068908691\n",
            "Loss:  5.137933731079102\n",
            "Loss:  5.851082801818848\n",
            "Loss:  5.614511489868164\n",
            "Loss:  6.058006286621094\n",
            "Loss:  6.516333103179932\n",
            "Loss:  7.066892147064209\n",
            "Loss:  6.569260597229004\n",
            "Loss:  7.6503190994262695\n",
            "Loss:  6.979348659515381\n",
            "Loss:  5.937023162841797\n",
            "Loss:  7.197112083435059\n",
            "Loss:  7.195293426513672\n",
            "Loss:  5.735687732696533\n",
            "Loss:  6.0655059814453125\n",
            "Loss:  6.665853977203369\n",
            "Loss:  7.772026062011719\n",
            "Loss:  7.8459625244140625\n",
            "Loss:  6.216048717498779\n",
            "Loss:  7.331967353820801\n",
            "Loss:  8.240804672241211\n",
            "Loss:  5.733773231506348\n",
            "Loss:  6.3333916664123535\n",
            "Loss:  7.08106803894043\n",
            "Loss:  6.648240089416504\n",
            "Loss:  7.431564807891846\n",
            "Loss:  7.943176746368408\n",
            "Loss:  7.1516618728637695\n",
            "Loss:  7.080475330352783\n",
            "Loss:  6.969155788421631\n",
            "Loss:  6.166225910186768\n",
            "Loss:  5.739071369171143\n",
            "Loss:  6.7409138679504395\n",
            "Loss:  6.200761795043945\n",
            "Loss:  7.467920780181885\n",
            "Loss:  5.784998893737793\n",
            "Loss:  5.969287395477295\n",
            "Loss:  8.0862398147583\n",
            "Loss:  5.19268274307251\n",
            "Loss:  6.119662761688232\n",
            "Loss:  5.834994792938232\n",
            "Loss:  6.288994312286377\n",
            "Loss:  5.811969757080078\n",
            "Loss:  6.0376739501953125\n",
            "Loss:  8.11553955078125\n",
            "Loss:  6.360477924346924\n",
            "Loss:  7.321925640106201\n",
            "Loss:  5.900277614593506\n",
            "Loss:  6.319053649902344\n",
            "Loss:  7.824731826782227\n",
            "Loss:  7.3043694496154785\n",
            "Loss:  7.673774719238281\n",
            "Loss:  6.347666263580322\n",
            "Loss:  7.888443946838379\n",
            "Loss:  6.583946228027344\n",
            "Loss:  6.641524791717529\n",
            "Loss:  5.9455246925354\n",
            "Loss:  6.584073066711426\n",
            "Loss:  6.659793853759766\n",
            "Loss:  7.6649651527404785\n",
            "Loss:  5.80517053604126\n",
            "Loss:  6.809570789337158\n",
            "Loss:  6.962978363037109\n",
            "Loss:  6.075509548187256\n",
            "Loss:  6.230177402496338\n",
            "Loss:  7.448623180389404\n",
            "Loss:  6.40101432800293\n",
            "Loss:  6.15189790725708\n",
            "Loss:  6.868905067443848\n",
            "Loss:  7.5596466064453125\n",
            "Loss:  7.537816524505615\n",
            "Loss:  6.961295127868652\n",
            "Loss:  6.696001052856445\n",
            "Loss:  7.271413326263428\n",
            "Loss:  7.30163049697876\n",
            "Loss:  5.692708492279053\n",
            "Loss:  6.2303385734558105\n",
            "Loss:  6.042741775512695\n",
            "Loss:  7.907778739929199\n",
            "Loss:  6.695235252380371\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}